{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ac86d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26558a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "fake_politifact = pd.read_csv('politifact_fake.csv')\n",
    "real_politifact = pd.read_csv('politifact_real.csv')\n",
    "fake_gossipcop = pd.read_csv('gossipcop_fake.csv')\n",
    "real_gossipcop = pd.read_csv('gossipcop_real.csv')\n",
    "\n",
    "# Assign labels: 0 for fake news, 1 for real news\n",
    "fake_politifact['label'] = 0\n",
    "real_politifact['label'] = 1\n",
    "fake_gossipcop['label'] = 0\n",
    "real_gossipcop['label'] = 1\n",
    "\n",
    "# Concatenate datasets\n",
    "data = pd.concat([fake_politifact, real_politifact, fake_gossipcop, real_gossipcop], axis=0, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201c9311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "\n",
    "    # Convert text to lowercase and tokenize words\n",
    "    words = text.lower().split()\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Join words back into a single string\n",
    "    cleaned_text = ' '.join(words)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "data['cleaned_title'] = data['title'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d6f8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets (80% train, 20% test)\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0018e88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef5451e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7de9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7b46b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d821d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59313ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_vectors(glove_file):\n",
    "    word_vectors = {}\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            word_vectors[word] = vector\n",
    "    return word_vectors\n",
    "\n",
    "glove_vectors = load_glove_vectors('glove.6B.50d.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc410ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_average_vectors(docs, vectorizer, word_vectors, dim=50):\n",
    "    # Calculate the tf-idf weights for the given documents\n",
    "    tfidf_matrix = vectorizer.fit_transform(docs)\n",
    "\n",
    "    # Initialize an empty matrix to store the weighted average vectors\n",
    "    weighted_vectors = np.zeros((len(docs), dim))\n",
    "\n",
    "    # Iterate through the documents and compute the weighted average vector for each\n",
    "    for i, doc in enumerate(docs):\n",
    "        words = doc.split()\n",
    "        weighted_sum = np.zeros(dim)\n",
    "        total_weight = 0\n",
    "\n",
    "        for word in words:\n",
    "            if word in word_vectors and word in vectorizer.vocabulary_:\n",
    "                vector = word_vectors[word]\n",
    "                weight = tfidf_matrix[i, vectorizer.vocabulary_[word]]\n",
    "                weighted_sum += weight * vector\n",
    "                total_weight += weight\n",
    "\n",
    "        if total_weight != 0:\n",
    "            weighted_vectors[i] = weighted_sum / total_weight\n",
    "\n",
    "    return weighted_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0327d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X_train = weighted_average_vectors(train_data['cleaned_title'], vectorizer, glove_vectors)\n",
    "X_test = weighted_average_vectors(test_data['cleaned_title'], vectorizer, glove_vectors)\n",
    "\n",
    "y_train = train_data['label'].values\n",
    "y_test = test_data['label'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4ce2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_autoencoder(input_dim, encoding_dim, hidden_layers):\n",
    "    # Encoder\n",
    "    encoder_input = Input(shape=(input_dim,))\n",
    "    encoder_layers = [Dense(hidden_layers[0], activation='relu')(encoder_input)]\n",
    "\n",
    "    for layer_size in hidden_layers[1:]:\n",
    "        encoder_layers.append(Dense(layer_size, activation='relu')(encoder_layers[-1]))\n",
    "\n",
    "    # Encoded representation\n",
    "    encoded = Dense(encoding_dim, activation='relu')(encoder_layers[-1])\n",
    "\n",
    "    # Decoder\n",
    "    decoder_layers = [Dense(hidden_layers[-1], activation='relu')(encoded)]\n",
    "\n",
    "    for layer_size in reversed(hidden_layers[:-1]):\n",
    "        decoder_layers.append(Dense(layer_size, activation='relu')(decoder_layers[-1]))\n",
    "\n",
    "    # Reconstruction\n",
    "    decoder_output = Dense(input_dim, activation='linear')(decoder_layers[-1])\n",
    "\n",
    "    # Build the autoencoder model\n",
    "    autoencoder = Model(encoder_input, decoder_output)\n",
    "    return autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0375989e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "encoding_dim = 32\n",
    "hidden_layers = [128, 64]\n",
    "\n",
    "autoencoder = create_autoencoder(input_dim, encoding_dim, hidden_layers)\n",
    "autoencoder.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5479f9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "loss = MeanSquaredError()\n",
    "autoencoder.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "# Train the model\n",
    "real_headlines = X_train[y_train == 1]  # filter real news headlines\n",
    "epochs = 300\n",
    "batch_size = 32\n",
    "autoencoder.fit(real_headlines, real_headlines, epochs=epochs, batch_size=batch_size, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aeca668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct headlines in the test set\n",
    "X_test_reconstructed = autoencoder.predict(X_test)\n",
    "\n",
    "# Calculate the reconstruction errors\n",
    "reconstruction_errors = np.mean((X_test - X_test_reconstructed)**2, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d113d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def find_optimal_threshold(errors, y_true):\n",
    "    min_diff = float('inf')\n",
    "    optimal_threshold = 0\n",
    "\n",
    "    for threshold in np.linspace(errors.min(), errors.max(), num=1000):\n",
    "        y_pred = (errors > threshold).astype(int)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        fpr = fp / (fp + tn)  # False Positive Rate\n",
    "        fnr = fn / (fn + tp)  # False Negative Rate\n",
    "\n",
    "        diff = abs(fpr - fnr)\n",
    "\n",
    "        if diff < min_diff:\n",
    "            min_diff = diff\n",
    "            optimal_threshold = threshold\n",
    "\n",
    "    return optimal_threshold\n",
    "\n",
    "threshold = find_optimal_threshold(reconstruction_errors, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cc79cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (reconstruction_errors > threshold).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2c548c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Autoencoder Classifier Metrics:\\nAccuracy: {accuracy}\\nPrecision: {precision}\\nRecall: {recall}\\nF1 Score: {f1}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc5dec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train the logistic regression model with an increased number of iterations\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_log_reg = log_reg.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy_log_reg = accuracy_score(y_test, y_pred_log_reg)\n",
    "precision_log_reg = precision_score(y_test, y_pred_log_reg)\n",
    "recall_log_reg = recall_score(y_test, y_pred_log_reg)\n",
    "f1_log_reg = f1_score(y_test, y_pred_log_reg)\n",
    "\n",
    "print(f\"Logistic Regression Metrics:\\nAccuracy: {accuracy_log_reg}\\nPrecision: {precision_log_reg}\\nRecall: {recall_log_reg}\\nF1 Score: {f1_log_reg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a48f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train the random forest model\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "precision_rf = precision_score(y_test, y_pred_rf)\n",
    "recall_rf = recall_score(y_test, y_pred_rf)\n",
    "f1_rf = f1_score(y_test, y_pred_rf)\n",
    "\n",
    "print(f\"Random Forest Metrics:\\nAccuracy: {accuracy_rf}\\nPrecision: {precision_rf}\\nRecall: {recall_rf}\\nF1 Score: {f1_rf}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2653ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad051b1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8446543b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318ce208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbe9f4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2151b644",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
