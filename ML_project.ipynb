{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c8ac86d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a26558a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "fake_politifact = pd.read_csv('politifact_fake.csv')\n",
    "real_politifact = pd.read_csv('politifact_real.csv')\n",
    "fake_gossipcop = pd.read_csv('gossipcop_fake.csv')\n",
    "real_gossipcop = pd.read_csv('gossipcop_real.csv')\n",
    "\n",
    "# Assign labels: 0 for fake news, 1 for real news\n",
    "fake_politifact['label'] = 0\n",
    "real_politifact['label'] = 1\n",
    "fake_gossipcop['label'] = 0\n",
    "real_gossipcop['label'] = 1\n",
    "\n",
    "# Concatenate datasets\n",
    "data = pd.concat([fake_politifact, real_politifact, fake_gossipcop, real_gossipcop], axis=0, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "201c9311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "\n",
    "    # Convert text to lowercase and tokenize words\n",
    "    words = text.lower().split()\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Join words back into a single string\n",
    "    cleaned_text = ' '.join(words)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "data['cleaned_title'] = data['title'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e7d6f8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets (80% train, 20% test)\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0018e88d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18556, 6)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8ef5451e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>news_url</th>\n",
       "      <th>title</th>\n",
       "      <th>tweet_ids</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9198</th>\n",
       "      <td>gossipcop-873539</td>\n",
       "      <td>https://www.etonline.com/news/223794_samuel_l_...</td>\n",
       "      <td>Samuel L. Jackson Schools James Corden in 'Dro...</td>\n",
       "      <td>897450356900114432\\t897450895167729664\\t897451...</td>\n",
       "      <td>1</td>\n",
       "      <td>samuel l jackson schools james corden drop mic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14985</th>\n",
       "      <td>gossipcop-853226</td>\n",
       "      <td>https://www.nydailynews.com/new-york/nyc-crime...</td>\n",
       "      <td>Alleged Taylor Swift stalker waited on roof of...</td>\n",
       "      <td>864840870826090498\\t864840973523623936\\t864841...</td>\n",
       "      <td>1</td>\n",
       "      <td>alleged taylor swift stalker waited roof manha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15100</th>\n",
       "      <td>gossipcop-850005</td>\n",
       "      <td>https://medium.com/@AlexChaney303138N/from-hop...</td>\n",
       "      <td>From Hopeful Australia Beginnings to a Tragic ...</td>\n",
       "      <td>864785411520049153\\t864786362523820038\\t864786...</td>\n",
       "      <td>1</td>\n",
       "      <td>hopeful australia beginnings tragic hollywood ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6186</th>\n",
       "      <td>gossipcop-4516431341</td>\n",
       "      <td>variety.com/2017/legit/awards/tony-awards-2017...</td>\n",
       "      <td>Tony Awards 2017: Complete Winners List</td>\n",
       "      <td>873270018309447680\\t873271704667205632\\t873272...</td>\n",
       "      <td>0</td>\n",
       "      <td>tony awards complete winners list</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11038</th>\n",
       "      <td>gossipcop-891466</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Disappearance_of...</td>\n",
       "      <td>Disappearance of Madeleine McCann</td>\n",
       "      <td>926802996821970944\\t926803044611829760\\t926804...</td>\n",
       "      <td>1</td>\n",
       "      <td>disappearance madeleine mccann</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id   \n",
       "9198       gossipcop-873539  \\\n",
       "14985      gossipcop-853226   \n",
       "15100      gossipcop-850005   \n",
       "6186   gossipcop-4516431341   \n",
       "11038      gossipcop-891466   \n",
       "\n",
       "                                                news_url   \n",
       "9198   https://www.etonline.com/news/223794_samuel_l_...  \\\n",
       "14985  https://www.nydailynews.com/new-york/nyc-crime...   \n",
       "15100  https://medium.com/@AlexChaney303138N/from-hop...   \n",
       "6186   variety.com/2017/legit/awards/tony-awards-2017...   \n",
       "11038  https://en.wikipedia.org/wiki/Disappearance_of...   \n",
       "\n",
       "                                                   title   \n",
       "9198   Samuel L. Jackson Schools James Corden in 'Dro...  \\\n",
       "14985  Alleged Taylor Swift stalker waited on roof of...   \n",
       "15100  From Hopeful Australia Beginnings to a Tragic ...   \n",
       "6186             Tony Awards 2017: Complete Winners List   \n",
       "11038                  Disappearance of Madeleine McCann   \n",
       "\n",
       "                                               tweet_ids  label   \n",
       "9198   897450356900114432\\t897450895167729664\\t897451...      1  \\\n",
       "14985  864840870826090498\\t864840973523623936\\t864841...      1   \n",
       "15100  864785411520049153\\t864786362523820038\\t864786...      1   \n",
       "6186   873270018309447680\\t873271704667205632\\t873272...      0   \n",
       "11038  926802996821970944\\t926803044611829760\\t926804...      1   \n",
       "\n",
       "                                           cleaned_title  \n",
       "9198   samuel l jackson schools james corden drop mic...  \n",
       "14985  alleged taylor swift stalker waited roof manha...  \n",
       "15100  hopeful australia beginnings tragic hollywood ...  \n",
       "6186                   tony awards complete winners list  \n",
       "11038                     disappearance madeleine mccann  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9a7de9c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4640, 6)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8a7b46b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>news_url</th>\n",
       "      <th>title</th>\n",
       "      <th>tweet_ids</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4291</th>\n",
       "      <td>gossipcop-3430607867</td>\n",
       "      <td>www.today.com/video/mariah-careys-twins-steal-...</td>\n",
       "      <td>Mariah Carey’s twins steal spotlight at Hollyw...</td>\n",
       "      <td>629276787600560128\\t629286113706795009\\t629326...</td>\n",
       "      <td>0</td>\n",
       "      <td>mariah carey twins steal spotlight hollywood w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14922</th>\n",
       "      <td>gossipcop-930854</td>\n",
       "      <td>https://people.com/tv/teen-mom-bombshells/</td>\n",
       "      <td>Teen Mom's Most Bombshell and Dramatic Moments</td>\n",
       "      <td>991304765010333696\\t991305082875662336\\t991305...</td>\n",
       "      <td>1</td>\n",
       "      <td>teen mom bombshell dramatic moments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19868</th>\n",
       "      <td>gossipcop-911475</td>\n",
       "      <td>https://www.vanityfair.com/hollywood/2018/02/s...</td>\n",
       "      <td>S.N.L.: Watch Natalie Portman Rap About Star W...</td>\n",
       "      <td>960156003277856768\\t960175040087150592\\t960178...</td>\n",
       "      <td>1</td>\n",
       "      <td>n l watch natalie portman rap star wars preque...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12669</th>\n",
       "      <td>gossipcop-847921</td>\n",
       "      <td>https://www.springfieldspringfield.co.uk/view_...</td>\n",
       "      <td>The Arrangement (2017) s02e02 Episode Script</td>\n",
       "      <td>859786149417504770\\t859786174889472000\\t859787...</td>\n",
       "      <td>1</td>\n",
       "      <td>arrangement e episode script</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12570</th>\n",
       "      <td>gossipcop-886689</td>\n",
       "      <td>https://en.paperblog.com/the-platinum-life-rec...</td>\n",
       "      <td>The Platinum Life Recap: The Ladies Take a Tri...</td>\n",
       "      <td>919764599016058880\\t919764869817266176\\t919765...</td>\n",
       "      <td>1</td>\n",
       "      <td>platinum life recap ladies take trip vegas aly...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id   \n",
       "4291   gossipcop-3430607867  \\\n",
       "14922      gossipcop-930854   \n",
       "19868      gossipcop-911475   \n",
       "12669      gossipcop-847921   \n",
       "12570      gossipcop-886689   \n",
       "\n",
       "                                                news_url   \n",
       "4291   www.today.com/video/mariah-careys-twins-steal-...  \\\n",
       "14922         https://people.com/tv/teen-mom-bombshells/   \n",
       "19868  https://www.vanityfair.com/hollywood/2018/02/s...   \n",
       "12669  https://www.springfieldspringfield.co.uk/view_...   \n",
       "12570  https://en.paperblog.com/the-platinum-life-rec...   \n",
       "\n",
       "                                                   title   \n",
       "4291   Mariah Carey’s twins steal spotlight at Hollyw...  \\\n",
       "14922     Teen Mom's Most Bombshell and Dramatic Moments   \n",
       "19868  S.N.L.: Watch Natalie Portman Rap About Star W...   \n",
       "12669       The Arrangement (2017) s02e02 Episode Script   \n",
       "12570  The Platinum Life Recap: The Ladies Take a Tri...   \n",
       "\n",
       "                                               tweet_ids  label   \n",
       "4291   629276787600560128\\t629286113706795009\\t629326...      0  \\\n",
       "14922  991304765010333696\\t991305082875662336\\t991305...      1   \n",
       "19868  960156003277856768\\t960175040087150592\\t960178...      1   \n",
       "12669  859786149417504770\\t859786174889472000\\t859787...      1   \n",
       "12570  919764599016058880\\t919764869817266176\\t919765...      1   \n",
       "\n",
       "                                           cleaned_title  \n",
       "4291   mariah carey twins steal spotlight hollywood w...  \n",
       "14922                teen mom bombshell dramatic moments  \n",
       "19868  n l watch natalie portman rap star wars preque...  \n",
       "12669                       arrangement e episode script  \n",
       "12570  platinum life recap ladies take trip vegas aly...  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72d2c10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "59313ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_vectors(glove_file):\n",
    "    word_vectors = {}\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            word_vectors[word] = vector\n",
    "    return word_vectors\n",
    "\n",
    "glove_vectors = load_glove_vectors('glove.6B.50d.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9fc410ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_average_vectors(docs, vectorizer, word_vectors, dim=50):\n",
    "    # Calculate the tf-idf weights for the given documents\n",
    "    tfidf_matrix = vectorizer.fit_transform(docs)\n",
    "\n",
    "    # Initialize an empty matrix to store the weighted average vectors\n",
    "    weighted_vectors = np.zeros((len(docs), dim))\n",
    "\n",
    "    # Iterate through the documents and compute the weighted average vector for each\n",
    "    for i, doc in enumerate(docs):\n",
    "        words = doc.split()\n",
    "        weighted_sum = np.zeros(dim)\n",
    "        total_weight = 0\n",
    "\n",
    "        for word in words:\n",
    "            if word in word_vectors and word in vectorizer.vocabulary_:\n",
    "                vector = word_vectors[word]\n",
    "                weight = tfidf_matrix[i, vectorizer.vocabulary_[word]]\n",
    "                weighted_sum += weight * vector\n",
    "                total_weight += weight\n",
    "\n",
    "        if total_weight != 0:\n",
    "            weighted_vectors[i] = weighted_sum / total_weight\n",
    "\n",
    "    return weighted_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f0327d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X_train = weighted_average_vectors(train_data['cleaned_title'], vectorizer, glove_vectors)\n",
    "X_test = weighted_average_vectors(test_data['cleaned_title'], vectorizer, glove_vectors)\n",
    "\n",
    "y_train = train_data['label'].values\n",
    "y_test = test_data['label'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4d4ce2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_autoencoder(input_dim, encoding_dim, hidden_layers):\n",
    "    # Encoder\n",
    "    encoder_input = Input(shape=(input_dim,))\n",
    "    encoder_layers = [Dense(hidden_layers[0], activation='relu')(encoder_input)]\n",
    "\n",
    "    for layer_size in hidden_layers[1:]:\n",
    "        encoder_layers.append(Dense(layer_size, activation='relu')(encoder_layers[-1]))\n",
    "\n",
    "    # Encoded representation\n",
    "    encoded = Dense(encoding_dim, activation='relu')(encoder_layers[-1])\n",
    "\n",
    "    # Decoder\n",
    "    decoder_layers = [Dense(hidden_layers[-1], activation='relu')(encoded)]\n",
    "\n",
    "    for layer_size in reversed(hidden_layers[:-1]):\n",
    "        decoder_layers.append(Dense(layer_size, activation='relu')(decoder_layers[-1]))\n",
    "\n",
    "    # Reconstruction\n",
    "    decoder_output = Dense(input_dim, activation='linear')(decoder_layers[-1])\n",
    "\n",
    "    # Build the autoencoder model\n",
    "    autoencoder = Model(encoder_input, decoder_output)\n",
    "    return autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0375989e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 50)]              0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 128)               6528      \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 50)                6450      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 33,746\n",
      "Trainable params: 33,746\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "encoding_dim = 32\n",
    "hidden_layers = [128, 64]\n",
    "\n",
    "autoencoder = create_autoencoder(input_dim, encoding_dim, hidden_layers)\n",
    "autoencoder.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "68e50968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "393/393 [==============================] - 1s 1ms/step - loss: 0.0511 - val_loss: 0.0330\n",
      "Epoch 2/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0268 - val_loss: 0.0266\n",
      "Epoch 3/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0231 - val_loss: 0.0245\n",
      "Epoch 4/100\n",
      "393/393 [==============================] - 0s 955us/step - loss: 0.0220 - val_loss: 0.0234\n",
      "Epoch 5/100\n",
      "393/393 [==============================] - 0s 894us/step - loss: 0.0214 - val_loss: 0.0230\n",
      "Epoch 6/100\n",
      "393/393 [==============================] - 0s 944us/step - loss: 0.0210 - val_loss: 0.0227\n",
      "Epoch 7/100\n",
      "393/393 [==============================] - 0s 961us/step - loss: 0.0207 - val_loss: 0.0222\n",
      "Epoch 8/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0204 - val_loss: 0.0222\n",
      "Epoch 9/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0203 - val_loss: 0.0217\n",
      "Epoch 10/100\n",
      "393/393 [==============================] - 0s 940us/step - loss: 0.0201 - val_loss: 0.0218\n",
      "Epoch 11/100\n",
      "393/393 [==============================] - 0s 960us/step - loss: 0.0200 - val_loss: 0.0214\n",
      "Epoch 12/100\n",
      "393/393 [==============================] - 0s 954us/step - loss: 0.0198 - val_loss: 0.0214\n",
      "Epoch 13/100\n",
      "393/393 [==============================] - 0s 986us/step - loss: 0.0197 - val_loss: 0.0213\n",
      "Epoch 14/100\n",
      "393/393 [==============================] - 0s 970us/step - loss: 0.0196 - val_loss: 0.0212\n",
      "Epoch 15/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0195 - val_loss: 0.0213\n",
      "Epoch 16/100\n",
      "393/393 [==============================] - 0s 980us/step - loss: 0.0193 - val_loss: 0.0210\n",
      "Epoch 17/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0193 - val_loss: 0.0208\n",
      "Epoch 18/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0192 - val_loss: 0.0206\n",
      "Epoch 19/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0191 - val_loss: 0.0207\n",
      "Epoch 20/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0190 - val_loss: 0.0203\n",
      "Epoch 21/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0189 - val_loss: 0.0210\n",
      "Epoch 22/100\n",
      "393/393 [==============================] - 0s 984us/step - loss: 0.0188 - val_loss: 0.0204\n",
      "Epoch 23/100\n",
      "393/393 [==============================] - 0s 951us/step - loss: 0.0187 - val_loss: 0.0207\n",
      "Epoch 24/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0187 - val_loss: 0.0204\n",
      "Epoch 25/100\n",
      "393/393 [==============================] - 0s 995us/step - loss: 0.0186 - val_loss: 0.0205\n",
      "Epoch 26/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0185 - val_loss: 0.0201\n",
      "Epoch 27/100\n",
      "393/393 [==============================] - 0s 979us/step - loss: 0.0185 - val_loss: 0.0200\n",
      "Epoch 28/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0184 - val_loss: 0.0200\n",
      "Epoch 29/100\n",
      "393/393 [==============================] - 0s 984us/step - loss: 0.0183 - val_loss: 0.0201\n",
      "Epoch 30/100\n",
      "393/393 [==============================] - 0s 981us/step - loss: 0.0182 - val_loss: 0.0200\n",
      "Epoch 31/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0182 - val_loss: 0.0197\n",
      "Epoch 32/100\n",
      "393/393 [==============================] - 0s 963us/step - loss: 0.0181 - val_loss: 0.0197\n",
      "Epoch 33/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0181 - val_loss: 0.0201\n",
      "Epoch 34/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0181 - val_loss: 0.0199\n",
      "Epoch 35/100\n",
      "393/393 [==============================] - 0s 960us/step - loss: 0.0180 - val_loss: 0.0197\n",
      "Epoch 36/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0179 - val_loss: 0.0195\n",
      "Epoch 37/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0179 - val_loss: 0.0197\n",
      "Epoch 38/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0177 - val_loss: 0.0192\n",
      "Epoch 39/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0171 - val_loss: 0.0186\n",
      "Epoch 40/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0169 - val_loss: 0.0187\n",
      "Epoch 41/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0168 - val_loss: 0.0185\n",
      "Epoch 42/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0168 - val_loss: 0.0185\n",
      "Epoch 43/100\n",
      "393/393 [==============================] - 0s 990us/step - loss: 0.0166 - val_loss: 0.0185\n",
      "Epoch 44/100\n",
      "393/393 [==============================] - 0s 938us/step - loss: 0.0166 - val_loss: 0.0184\n",
      "Epoch 45/100\n",
      "393/393 [==============================] - 0s 998us/step - loss: 0.0165 - val_loss: 0.0182\n",
      "Epoch 46/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0165 - val_loss: 0.0185\n",
      "Epoch 47/100\n",
      "393/393 [==============================] - 0s 982us/step - loss: 0.0164 - val_loss: 0.0183\n",
      "Epoch 48/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0164 - val_loss: 0.0183\n",
      "Epoch 49/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0163 - val_loss: 0.0181\n",
      "Epoch 50/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0163 - val_loss: 0.0180\n",
      "Epoch 51/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0163 - val_loss: 0.0182\n",
      "Epoch 52/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0163 - val_loss: 0.0181\n",
      "Epoch 53/100\n",
      "393/393 [==============================] - 0s 999us/step - loss: 0.0162 - val_loss: 0.0180\n",
      "Epoch 54/100\n",
      "393/393 [==============================] - 0s 960us/step - loss: 0.0162 - val_loss: 0.0183\n",
      "Epoch 55/100\n",
      "393/393 [==============================] - 0s 944us/step - loss: 0.0161 - val_loss: 0.0180\n",
      "Epoch 56/100\n",
      "393/393 [==============================] - 0s 975us/step - loss: 0.0161 - val_loss: 0.0181\n",
      "Epoch 57/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0161 - val_loss: 0.0181\n",
      "Epoch 58/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0161 - val_loss: 0.0181\n",
      "Epoch 59/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0160 - val_loss: 0.0181\n",
      "Epoch 60/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0160 - val_loss: 0.0179\n",
      "Epoch 61/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0160 - val_loss: 0.0178\n",
      "Epoch 62/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0159 - val_loss: 0.0177\n",
      "Epoch 63/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0159 - val_loss: 0.0180\n",
      "Epoch 64/100\n",
      "393/393 [==============================] - 0s 967us/step - loss: 0.0159 - val_loss: 0.0177\n",
      "Epoch 65/100\n",
      "393/393 [==============================] - 0s 969us/step - loss: 0.0158 - val_loss: 0.0180\n",
      "Epoch 66/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0159 - val_loss: 0.0178\n",
      "Epoch 67/100\n",
      "393/393 [==============================] - 0s 996us/step - loss: 0.0158 - val_loss: 0.0178\n",
      "Epoch 68/100\n",
      "393/393 [==============================] - 0s 949us/step - loss: 0.0158 - val_loss: 0.0178\n",
      "Epoch 69/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0158 - val_loss: 0.0180\n",
      "Epoch 70/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0158 - val_loss: 0.0180\n",
      "Epoch 71/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0157 - val_loss: 0.0178\n",
      "Epoch 72/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0158 - val_loss: 0.0179\n",
      "Epoch 73/100\n",
      "393/393 [==============================] - 0s 943us/step - loss: 0.0157 - val_loss: 0.0179\n",
      "Epoch 74/100\n",
      "393/393 [==============================] - 0s 966us/step - loss: 0.0157 - val_loss: 0.0180\n",
      "Epoch 75/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0156 - val_loss: 0.0178\n",
      "Epoch 76/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0156 - val_loss: 0.0177\n",
      "Epoch 77/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0156 - val_loss: 0.0179\n",
      "Epoch 78/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0156 - val_loss: 0.0176\n",
      "Epoch 79/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0155 - val_loss: 0.0176\n",
      "Epoch 80/100\n",
      "393/393 [==============================] - 0s 989us/step - loss: 0.0155 - val_loss: 0.0177\n",
      "Epoch 81/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0155 - val_loss: 0.0175\n",
      "Epoch 82/100\n",
      "393/393 [==============================] - 0s 973us/step - loss: 0.0155 - val_loss: 0.0177\n",
      "Epoch 83/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0155 - val_loss: 0.0175\n",
      "Epoch 84/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0155 - val_loss: 0.0176\n",
      "Epoch 85/100\n",
      "393/393 [==============================] - 0s 984us/step - loss: 0.0155 - val_loss: 0.0178\n",
      "Epoch 86/100\n",
      "393/393 [==============================] - 0s 949us/step - loss: 0.0154 - val_loss: 0.0179\n",
      "Epoch 87/100\n",
      "393/393 [==============================] - 0s 971us/step - loss: 0.0154 - val_loss: 0.0176\n",
      "Epoch 88/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0154 - val_loss: 0.0176\n",
      "Epoch 89/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0154 - val_loss: 0.0175\n",
      "Epoch 90/100\n",
      "393/393 [==============================] - 0s 978us/step - loss: 0.0153 - val_loss: 0.0176\n",
      "Epoch 91/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0154 - val_loss: 0.0176\n",
      "Epoch 92/100\n",
      "393/393 [==============================] - 0s 992us/step - loss: 0.0153 - val_loss: 0.0178\n",
      "Epoch 93/100\n",
      "393/393 [==============================] - 0s 933us/step - loss: 0.0153 - val_loss: 0.0177\n",
      "Epoch 94/100\n",
      "393/393 [==============================] - 0s 922us/step - loss: 0.0153 - val_loss: 0.0175\n",
      "Epoch 95/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0153 - val_loss: 0.0178\n",
      "Epoch 96/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0153 - val_loss: 0.0175\n",
      "Epoch 97/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0153 - val_loss: 0.0177\n",
      "Epoch 98/100\n",
      "393/393 [==============================] - 0s 949us/step - loss: 0.0153 - val_loss: 0.0175\n",
      "Epoch 99/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0153 - val_loss: 0.0174\n",
      "Epoch 100/100\n",
      "393/393 [==============================] - 0s 955us/step - loss: 0.0152 - val_loss: 0.0174\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2db35945ba0>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "loss = MeanSquaredError()\n",
    "autoencoder.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "# Train the model\n",
    "real_headlines = X_train[y_train == 1]  # filter real news headlines\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "autoencoder.fit(real_headlines, real_headlines, epochs=epochs, batch_size=batch_size, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "139df1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145/145 [==============================] - 0s 672us/step\n"
     ]
    }
   ],
   "source": [
    "# Reconstruct headlines in the test set\n",
    "X_test_reconstructed = autoencoder.predict(X_test)\n",
    "\n",
    "# Calculate the reconstruction errors\n",
    "reconstruction_errors = np.mean((X_test - X_test_reconstructed)**2, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1332c86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def find_optimal_threshold(errors, y_true):\n",
    "    min_diff = float('inf')\n",
    "    optimal_threshold = 0\n",
    "\n",
    "    for threshold in np.linspace(errors.min(), errors.max(), num=1000):\n",
    "        y_pred = (errors > threshold).astype(int)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        fpr = fp / (fp + tn)  # False Positive Rate\n",
    "        fnr = fn / (fn + tp)  # False Negative Rate\n",
    "\n",
    "        diff = abs(fpr - fnr)\n",
    "\n",
    "        if diff < min_diff:\n",
    "            min_diff = diff\n",
    "            optimal_threshold = threshold\n",
    "\n",
    "    return optimal_threshold\n",
    "\n",
    "threshold = find_optimal_threshold(reconstruction_errors, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0e631428",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (reconstruction_errors > threshold).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7fcd0e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder Classifier Metrics:\n",
      "Accuracy: 0.5174568965517241\n",
      "Precision: 0.7593448131037379\n",
      "Recall: 0.5204375359815774\n",
      "F1 Score: 0.6175918018787361\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Autoencoder Classifier Metrics:\\nAccuracy: {accuracy}\\nPrecision: {precision}\\nRecall: {recall}\\nF1 Score: {f1}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5be150bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Metrics:\n",
      "Accuracy: 0.7676724137931035\n",
      "Precision: 0.7801683816651076\n",
      "Recall: 0.9602763385146805\n",
      "F1 Score: 0.8609032258064516\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train the logistic regression model with an increased number of iterations\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_log_reg = log_reg.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy_log_reg = accuracy_score(y_test, y_pred_log_reg)\n",
    "precision_log_reg = precision_score(y_test, y_pred_log_reg)\n",
    "recall_log_reg = recall_score(y_test, y_pred_log_reg)\n",
    "f1_log_reg = f1_score(y_test, y_pred_log_reg)\n",
    "\n",
    "print(f\"Logistic Regression Metrics:\\nAccuracy: {accuracy_log_reg}\\nPrecision: {precision_log_reg}\\nRecall: {recall_log_reg}\\nF1 Score: {f1_log_reg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9ac0f01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Metrics:\n",
      "Accuracy: 0.7887931034482759\n",
      "Precision: 0.7873271889400921\n",
      "Recall: 0.9835924006908463\n",
      "F1 Score: 0.8745840798566675\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train the random forest model\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "precision_rf = precision_score(y_test, y_pred_rf)\n",
    "recall_rf = recall_score(y_test, y_pred_rf)\n",
    "f1_rf = f1_score(y_test, y_pred_rf)\n",
    "\n",
    "print(f\"Random Forest Metrics:\\nAccuracy: {accuracy_rf}\\nPrecision: {precision_rf}\\nRecall: {recall_rf}\\nF1 Score: {f1_rf}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "93fc1867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the abcnews dataset\n",
    "abcnews_data = pd.read_csv('abcnews-date-text.csv')\n",
    "\n",
    "# Clean the headlines\n",
    "abcnews_data['cleaned_title'] = abcnews_data['headline_text'].apply(clean_text)\n",
    "\n",
    "# Assign labels (all real news)\n",
    "abcnews_data['label'] = 1\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_abcnews, val_abcnews = train_test_split(abcnews_data, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "14c44302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "33047/33047 [==============================] - 31s 941us/step - loss: 0.0228 - val_loss: 0.0216\n",
      "Epoch 2/100\n",
      "33047/33047 [==============================] - 31s 938us/step - loss: 0.0213 - val_loss: 0.0211\n",
      "Epoch 3/100\n",
      "33047/33047 [==============================] - 31s 935us/step - loss: 0.0207 - val_loss: 0.0204\n",
      "Epoch 4/100\n",
      "32909/33047 [============================>.] - ETA: 0s - loss: 0.0200"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[132], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Train the autoencoder on the real headlines from the abcnews training set\u001b[39;00m\n\u001b[0;32m      8\u001b[0m real_headlines_abc \u001b[38;5;241m=\u001b[39m X_train_abc[y_train_abc \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m----> 9\u001b[0m \u001b[43mautoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_headlines_abc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_headlines_abc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1677\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1678\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1679\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1682\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1683\u001b[0m ):\n\u001b[0;32m   1684\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1685\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1687\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    923\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    924\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    925\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 926\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    928\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    929\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m    142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1758\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    387\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    389\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    390\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    394\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Calculate the tf-idf weighted GloVe embeddings for the abcnews training set\n",
    "X_train_abc = weighted_average_vectors(train_abcnews['cleaned_title'], vectorizer, glove_vectors)\n",
    "\n",
    "# Get the corresponding labels\n",
    "y_train_abc = train_abcnews['label'].values\n",
    "\n",
    "# Train the autoencoder on the real headlines from the abcnews training set\n",
    "real_headlines_abc = X_train_abc[y_train_abc == 1]\n",
    "autoencoder.fit(real_headlines_abc, real_headlines_abc, epochs=100, batch_size=32, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee4de1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b68eaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
