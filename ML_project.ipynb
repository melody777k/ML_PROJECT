{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c8ac86d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a26558a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "fake_politifact = pd.read_csv('politifact_fake.csv')\n",
    "real_politifact = pd.read_csv('politifact_real.csv')\n",
    "fake_gossipcop = pd.read_csv('gossipcop_fake.csv')\n",
    "real_gossipcop = pd.read_csv('gossipcop_real.csv')\n",
    "\n",
    "# Assign labels: 0 for fake news, 1 for real news\n",
    "fake_politifact['label'] = 0\n",
    "real_politifact['label'] = 1\n",
    "fake_gossipcop['label'] = 0\n",
    "real_gossipcop['label'] = 1\n",
    "\n",
    "# Concatenate datasets\n",
    "data = pd.concat([fake_politifact, real_politifact, fake_gossipcop, real_gossipcop], axis=0, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "201c9311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "\n",
    "    # Convert text to lowercase and tokenize words\n",
    "    words = text.lower().split()\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Join words back into a single string\n",
    "    cleaned_text = ' '.join(words)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "data['cleaned_title'] = data['title'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e7d6f8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets (80% train, 20% test)\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0018e88d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18556, 6)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8ef5451e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>news_url</th>\n",
       "      <th>title</th>\n",
       "      <th>tweet_ids</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9198</th>\n",
       "      <td>gossipcop-873539</td>\n",
       "      <td>https://www.etonline.com/news/223794_samuel_l_...</td>\n",
       "      <td>Samuel L. Jackson Schools James Corden in 'Dro...</td>\n",
       "      <td>897450356900114432\\t897450895167729664\\t897451...</td>\n",
       "      <td>1</td>\n",
       "      <td>samuel l jackson schools james corden drop mic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14985</th>\n",
       "      <td>gossipcop-853226</td>\n",
       "      <td>https://www.nydailynews.com/new-york/nyc-crime...</td>\n",
       "      <td>Alleged Taylor Swift stalker waited on roof of...</td>\n",
       "      <td>864840870826090498\\t864840973523623936\\t864841...</td>\n",
       "      <td>1</td>\n",
       "      <td>alleged taylor swift stalker waited roof manha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15100</th>\n",
       "      <td>gossipcop-850005</td>\n",
       "      <td>https://medium.com/@AlexChaney303138N/from-hop...</td>\n",
       "      <td>From Hopeful Australia Beginnings to a Tragic ...</td>\n",
       "      <td>864785411520049153\\t864786362523820038\\t864786...</td>\n",
       "      <td>1</td>\n",
       "      <td>hopeful australia beginnings tragic hollywood ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6186</th>\n",
       "      <td>gossipcop-4516431341</td>\n",
       "      <td>variety.com/2017/legit/awards/tony-awards-2017...</td>\n",
       "      <td>Tony Awards 2017: Complete Winners List</td>\n",
       "      <td>873270018309447680\\t873271704667205632\\t873272...</td>\n",
       "      <td>0</td>\n",
       "      <td>tony awards complete winners list</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11038</th>\n",
       "      <td>gossipcop-891466</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Disappearance_of...</td>\n",
       "      <td>Disappearance of Madeleine McCann</td>\n",
       "      <td>926802996821970944\\t926803044611829760\\t926804...</td>\n",
       "      <td>1</td>\n",
       "      <td>disappearance madeleine mccann</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id   \n",
       "9198       gossipcop-873539  \\\n",
       "14985      gossipcop-853226   \n",
       "15100      gossipcop-850005   \n",
       "6186   gossipcop-4516431341   \n",
       "11038      gossipcop-891466   \n",
       "\n",
       "                                                news_url   \n",
       "9198   https://www.etonline.com/news/223794_samuel_l_...  \\\n",
       "14985  https://www.nydailynews.com/new-york/nyc-crime...   \n",
       "15100  https://medium.com/@AlexChaney303138N/from-hop...   \n",
       "6186   variety.com/2017/legit/awards/tony-awards-2017...   \n",
       "11038  https://en.wikipedia.org/wiki/Disappearance_of...   \n",
       "\n",
       "                                                   title   \n",
       "9198   Samuel L. Jackson Schools James Corden in 'Dro...  \\\n",
       "14985  Alleged Taylor Swift stalker waited on roof of...   \n",
       "15100  From Hopeful Australia Beginnings to a Tragic ...   \n",
       "6186             Tony Awards 2017: Complete Winners List   \n",
       "11038                  Disappearance of Madeleine McCann   \n",
       "\n",
       "                                               tweet_ids  label   \n",
       "9198   897450356900114432\\t897450895167729664\\t897451...      1  \\\n",
       "14985  864840870826090498\\t864840973523623936\\t864841...      1   \n",
       "15100  864785411520049153\\t864786362523820038\\t864786...      1   \n",
       "6186   873270018309447680\\t873271704667205632\\t873272...      0   \n",
       "11038  926802996821970944\\t926803044611829760\\t926804...      1   \n",
       "\n",
       "                                           cleaned_title  \n",
       "9198   samuel l jackson schools james corden drop mic...  \n",
       "14985  alleged taylor swift stalker waited roof manha...  \n",
       "15100  hopeful australia beginnings tragic hollywood ...  \n",
       "6186                   tony awards complete winners list  \n",
       "11038                     disappearance madeleine mccann  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9a7de9c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4640, 6)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8a7b46b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>news_url</th>\n",
       "      <th>title</th>\n",
       "      <th>tweet_ids</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4291</th>\n",
       "      <td>gossipcop-3430607867</td>\n",
       "      <td>www.today.com/video/mariah-careys-twins-steal-...</td>\n",
       "      <td>Mariah Carey’s twins steal spotlight at Hollyw...</td>\n",
       "      <td>629276787600560128\\t629286113706795009\\t629326...</td>\n",
       "      <td>0</td>\n",
       "      <td>mariah carey twins steal spotlight hollywood w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14922</th>\n",
       "      <td>gossipcop-930854</td>\n",
       "      <td>https://people.com/tv/teen-mom-bombshells/</td>\n",
       "      <td>Teen Mom's Most Bombshell and Dramatic Moments</td>\n",
       "      <td>991304765010333696\\t991305082875662336\\t991305...</td>\n",
       "      <td>1</td>\n",
       "      <td>teen mom bombshell dramatic moments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19868</th>\n",
       "      <td>gossipcop-911475</td>\n",
       "      <td>https://www.vanityfair.com/hollywood/2018/02/s...</td>\n",
       "      <td>S.N.L.: Watch Natalie Portman Rap About Star W...</td>\n",
       "      <td>960156003277856768\\t960175040087150592\\t960178...</td>\n",
       "      <td>1</td>\n",
       "      <td>n l watch natalie portman rap star wars preque...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12669</th>\n",
       "      <td>gossipcop-847921</td>\n",
       "      <td>https://www.springfieldspringfield.co.uk/view_...</td>\n",
       "      <td>The Arrangement (2017) s02e02 Episode Script</td>\n",
       "      <td>859786149417504770\\t859786174889472000\\t859787...</td>\n",
       "      <td>1</td>\n",
       "      <td>arrangement e episode script</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12570</th>\n",
       "      <td>gossipcop-886689</td>\n",
       "      <td>https://en.paperblog.com/the-platinum-life-rec...</td>\n",
       "      <td>The Platinum Life Recap: The Ladies Take a Tri...</td>\n",
       "      <td>919764599016058880\\t919764869817266176\\t919765...</td>\n",
       "      <td>1</td>\n",
       "      <td>platinum life recap ladies take trip vegas aly...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id   \n",
       "4291   gossipcop-3430607867  \\\n",
       "14922      gossipcop-930854   \n",
       "19868      gossipcop-911475   \n",
       "12669      gossipcop-847921   \n",
       "12570      gossipcop-886689   \n",
       "\n",
       "                                                news_url   \n",
       "4291   www.today.com/video/mariah-careys-twins-steal-...  \\\n",
       "14922         https://people.com/tv/teen-mom-bombshells/   \n",
       "19868  https://www.vanityfair.com/hollywood/2018/02/s...   \n",
       "12669  https://www.springfieldspringfield.co.uk/view_...   \n",
       "12570  https://en.paperblog.com/the-platinum-life-rec...   \n",
       "\n",
       "                                                   title   \n",
       "4291   Mariah Carey’s twins steal spotlight at Hollyw...  \\\n",
       "14922     Teen Mom's Most Bombshell and Dramatic Moments   \n",
       "19868  S.N.L.: Watch Natalie Portman Rap About Star W...   \n",
       "12669       The Arrangement (2017) s02e02 Episode Script   \n",
       "12570  The Platinum Life Recap: The Ladies Take a Tri...   \n",
       "\n",
       "                                               tweet_ids  label   \n",
       "4291   629276787600560128\\t629286113706795009\\t629326...      0  \\\n",
       "14922  991304765010333696\\t991305082875662336\\t991305...      1   \n",
       "19868  960156003277856768\\t960175040087150592\\t960178...      1   \n",
       "12669  859786149417504770\\t859786174889472000\\t859787...      1   \n",
       "12570  919764599016058880\\t919764869817266176\\t919765...      1   \n",
       "\n",
       "                                           cleaned_title  \n",
       "4291   mariah carey twins steal spotlight hollywood w...  \n",
       "14922                teen mom bombshell dramatic moments  \n",
       "19868  n l watch natalie portman rap star wars preque...  \n",
       "12669                       arrangement e episode script  \n",
       "12570  platinum life recap ladies take trip vegas aly...  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c35488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "59313ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_vectors(glove_file):\n",
    "    word_vectors = {}\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            word_vectors[word] = vector\n",
    "    return word_vectors\n",
    "\n",
    "glove_vectors = load_glove_vectors('glove.6B.50d.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9fc410ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_average_vectors(docs, vectorizer, word_vectors, dim=50):\n",
    "    # Calculate the tf-idf weights for the given documents\n",
    "    tfidf_matrix = vectorizer.fit_transform(docs)\n",
    "\n",
    "    # Initialize an empty matrix to store the weighted average vectors\n",
    "    weighted_vectors = np.zeros((len(docs), dim))\n",
    "\n",
    "    # Iterate through the documents and compute the weighted average vector for each\n",
    "    for i, doc in enumerate(docs):\n",
    "        words = doc.split()\n",
    "        weighted_sum = np.zeros(dim)\n",
    "        total_weight = 0\n",
    "\n",
    "        for word in words:\n",
    "            if word in word_vectors and word in vectorizer.vocabulary_:\n",
    "                vector = word_vectors[word]\n",
    "                weight = tfidf_matrix[i, vectorizer.vocabulary_[word]]\n",
    "                weighted_sum += weight * vector\n",
    "                total_weight += weight\n",
    "\n",
    "        if total_weight != 0:\n",
    "            weighted_vectors[i] = weighted_sum / total_weight\n",
    "\n",
    "    return weighted_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f0327d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X_train = weighted_average_vectors(train_data['cleaned_title'], vectorizer, glove_vectors)\n",
    "X_test = weighted_average_vectors(test_data['cleaned_title'], vectorizer, glove_vectors)\n",
    "\n",
    "y_train = train_data['label'].values\n",
    "y_test = test_data['label'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4d4ce2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_autoencoder(input_dim, encoding_dim, hidden_layers):\n",
    "    # Encoder\n",
    "    encoder_input = Input(shape=(input_dim,))\n",
    "    encoder_layers = [Dense(hidden_layers[0], activation='relu')(encoder_input)]\n",
    "\n",
    "    for layer_size in hidden_layers[1:]:\n",
    "        encoder_layers.append(Dense(layer_size, activation='relu')(encoder_layers[-1]))\n",
    "\n",
    "    # Encoded representation\n",
    "    encoded = Dense(encoding_dim, activation='relu')(encoder_layers[-1])\n",
    "\n",
    "    # Decoder\n",
    "    decoder_layers = [Dense(hidden_layers[-1], activation='relu')(encoded)]\n",
    "\n",
    "    for layer_size in reversed(hidden_layers[:-1]):\n",
    "        decoder_layers.append(Dense(layer_size, activation='relu')(decoder_layers[-1]))\n",
    "\n",
    "    # Reconstruction\n",
    "    decoder_output = Dense(input_dim, activation='linear')(decoder_layers[-1])\n",
    "\n",
    "    # Build the autoencoder model\n",
    "    autoencoder = Model(encoder_input, decoder_output)\n",
    "    return autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0375989e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 50)]              0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 128)               6528      \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 50)                6450      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 33,746\n",
      "Trainable params: 33,746\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "encoding_dim = 32\n",
    "hidden_layers = [128, 64]\n",
    "\n",
    "autoencoder = create_autoencoder(input_dim, encoding_dim, hidden_layers)\n",
    "autoencoder.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "fce6155c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "393/393 [==============================] - 1s 1ms/step - loss: 0.0511 - val_loss: 0.0330\n",
      "Epoch 2/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0268 - val_loss: 0.0266\n",
      "Epoch 3/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0231 - val_loss: 0.0245\n",
      "Epoch 4/100\n",
      "393/393 [==============================] - 0s 955us/step - loss: 0.0220 - val_loss: 0.0234\n",
      "Epoch 5/100\n",
      "393/393 [==============================] - 0s 894us/step - loss: 0.0214 - val_loss: 0.0230\n",
      "Epoch 6/100\n",
      "393/393 [==============================] - 0s 944us/step - loss: 0.0210 - val_loss: 0.0227\n",
      "Epoch 7/100\n",
      "393/393 [==============================] - 0s 961us/step - loss: 0.0207 - val_loss: 0.0222\n",
      "Epoch 8/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0204 - val_loss: 0.0222\n",
      "Epoch 9/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0203 - val_loss: 0.0217\n",
      "Epoch 10/100\n",
      "393/393 [==============================] - 0s 940us/step - loss: 0.0201 - val_loss: 0.0218\n",
      "Epoch 11/100\n",
      "393/393 [==============================] - 0s 960us/step - loss: 0.0200 - val_loss: 0.0214\n",
      "Epoch 12/100\n",
      "393/393 [==============================] - 0s 954us/step - loss: 0.0198 - val_loss: 0.0214\n",
      "Epoch 13/100\n",
      "393/393 [==============================] - 0s 986us/step - loss: 0.0197 - val_loss: 0.0213\n",
      "Epoch 14/100\n",
      "393/393 [==============================] - 0s 970us/step - loss: 0.0196 - val_loss: 0.0212\n",
      "Epoch 15/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0195 - val_loss: 0.0213\n",
      "Epoch 16/100\n",
      "393/393 [==============================] - 0s 980us/step - loss: 0.0193 - val_loss: 0.0210\n",
      "Epoch 17/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0193 - val_loss: 0.0208\n",
      "Epoch 18/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0192 - val_loss: 0.0206\n",
      "Epoch 19/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0191 - val_loss: 0.0207\n",
      "Epoch 20/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0190 - val_loss: 0.0203\n",
      "Epoch 21/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0189 - val_loss: 0.0210\n",
      "Epoch 22/100\n",
      "393/393 [==============================] - 0s 984us/step - loss: 0.0188 - val_loss: 0.0204\n",
      "Epoch 23/100\n",
      "393/393 [==============================] - 0s 951us/step - loss: 0.0187 - val_loss: 0.0207\n",
      "Epoch 24/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0187 - val_loss: 0.0204\n",
      "Epoch 25/100\n",
      "393/393 [==============================] - 0s 995us/step - loss: 0.0186 - val_loss: 0.0205\n",
      "Epoch 26/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0185 - val_loss: 0.0201\n",
      "Epoch 27/100\n",
      "393/393 [==============================] - 0s 979us/step - loss: 0.0185 - val_loss: 0.0200\n",
      "Epoch 28/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0184 - val_loss: 0.0200\n",
      "Epoch 29/100\n",
      "393/393 [==============================] - 0s 984us/step - loss: 0.0183 - val_loss: 0.0201\n",
      "Epoch 30/100\n",
      "393/393 [==============================] - 0s 981us/step - loss: 0.0182 - val_loss: 0.0200\n",
      "Epoch 31/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0182 - val_loss: 0.0197\n",
      "Epoch 32/100\n",
      "393/393 [==============================] - 0s 963us/step - loss: 0.0181 - val_loss: 0.0197\n",
      "Epoch 33/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0181 - val_loss: 0.0201\n",
      "Epoch 34/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0181 - val_loss: 0.0199\n",
      "Epoch 35/100\n",
      "393/393 [==============================] - 0s 960us/step - loss: 0.0180 - val_loss: 0.0197\n",
      "Epoch 36/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0179 - val_loss: 0.0195\n",
      "Epoch 37/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0179 - val_loss: 0.0197\n",
      "Epoch 38/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0177 - val_loss: 0.0192\n",
      "Epoch 39/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0171 - val_loss: 0.0186\n",
      "Epoch 40/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0169 - val_loss: 0.0187\n",
      "Epoch 41/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0168 - val_loss: 0.0185\n",
      "Epoch 42/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0168 - val_loss: 0.0185\n",
      "Epoch 43/100\n",
      "393/393 [==============================] - 0s 990us/step - loss: 0.0166 - val_loss: 0.0185\n",
      "Epoch 44/100\n",
      "393/393 [==============================] - 0s 938us/step - loss: 0.0166 - val_loss: 0.0184\n",
      "Epoch 45/100\n",
      "393/393 [==============================] - 0s 998us/step - loss: 0.0165 - val_loss: 0.0182\n",
      "Epoch 46/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0165 - val_loss: 0.0185\n",
      "Epoch 47/100\n",
      "393/393 [==============================] - 0s 982us/step - loss: 0.0164 - val_loss: 0.0183\n",
      "Epoch 48/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0164 - val_loss: 0.0183\n",
      "Epoch 49/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0163 - val_loss: 0.0181\n",
      "Epoch 50/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0163 - val_loss: 0.0180\n",
      "Epoch 51/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0163 - val_loss: 0.0182\n",
      "Epoch 52/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0163 - val_loss: 0.0181\n",
      "Epoch 53/100\n",
      "393/393 [==============================] - 0s 999us/step - loss: 0.0162 - val_loss: 0.0180\n",
      "Epoch 54/100\n",
      "393/393 [==============================] - 0s 960us/step - loss: 0.0162 - val_loss: 0.0183\n",
      "Epoch 55/100\n",
      "393/393 [==============================] - 0s 944us/step - loss: 0.0161 - val_loss: 0.0180\n",
      "Epoch 56/100\n",
      "393/393 [==============================] - 0s 975us/step - loss: 0.0161 - val_loss: 0.0181\n",
      "Epoch 57/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0161 - val_loss: 0.0181\n",
      "Epoch 58/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0161 - val_loss: 0.0181\n",
      "Epoch 59/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0160 - val_loss: 0.0181\n",
      "Epoch 60/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0160 - val_loss: 0.0179\n",
      "Epoch 61/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0160 - val_loss: 0.0178\n",
      "Epoch 62/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0159 - val_loss: 0.0177\n",
      "Epoch 63/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0159 - val_loss: 0.0180\n",
      "Epoch 64/100\n",
      "393/393 [==============================] - 0s 967us/step - loss: 0.0159 - val_loss: 0.0177\n",
      "Epoch 65/100\n",
      "393/393 [==============================] - 0s 969us/step - loss: 0.0158 - val_loss: 0.0180\n",
      "Epoch 66/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0159 - val_loss: 0.0178\n",
      "Epoch 67/100\n",
      "393/393 [==============================] - 0s 996us/step - loss: 0.0158 - val_loss: 0.0178\n",
      "Epoch 68/100\n",
      "393/393 [==============================] - 0s 949us/step - loss: 0.0158 - val_loss: 0.0178\n",
      "Epoch 69/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0158 - val_loss: 0.0180\n",
      "Epoch 70/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0158 - val_loss: 0.0180\n",
      "Epoch 71/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0157 - val_loss: 0.0178\n",
      "Epoch 72/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0158 - val_loss: 0.0179\n",
      "Epoch 73/100\n",
      "393/393 [==============================] - 0s 943us/step - loss: 0.0157 - val_loss: 0.0179\n",
      "Epoch 74/100\n",
      "393/393 [==============================] - 0s 966us/step - loss: 0.0157 - val_loss: 0.0180\n",
      "Epoch 75/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0156 - val_loss: 0.0178\n",
      "Epoch 76/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0156 - val_loss: 0.0177\n",
      "Epoch 77/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0156 - val_loss: 0.0179\n",
      "Epoch 78/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0156 - val_loss: 0.0176\n",
      "Epoch 79/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0155 - val_loss: 0.0176\n",
      "Epoch 80/100\n",
      "393/393 [==============================] - 0s 989us/step - loss: 0.0155 - val_loss: 0.0177\n",
      "Epoch 81/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0155 - val_loss: 0.0175\n",
      "Epoch 82/100\n",
      "393/393 [==============================] - 0s 973us/step - loss: 0.0155 - val_loss: 0.0177\n",
      "Epoch 83/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0155 - val_loss: 0.0175\n",
      "Epoch 84/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0155 - val_loss: 0.0176\n",
      "Epoch 85/100\n",
      "393/393 [==============================] - 0s 984us/step - loss: 0.0155 - val_loss: 0.0178\n",
      "Epoch 86/100\n",
      "393/393 [==============================] - 0s 949us/step - loss: 0.0154 - val_loss: 0.0179\n",
      "Epoch 87/100\n",
      "393/393 [==============================] - 0s 971us/step - loss: 0.0154 - val_loss: 0.0176\n",
      "Epoch 88/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0154 - val_loss: 0.0176\n",
      "Epoch 89/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0154 - val_loss: 0.0175\n",
      "Epoch 90/100\n",
      "393/393 [==============================] - 0s 978us/step - loss: 0.0153 - val_loss: 0.0176\n",
      "Epoch 91/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0154 - val_loss: 0.0176\n",
      "Epoch 92/100\n",
      "393/393 [==============================] - 0s 992us/step - loss: 0.0153 - val_loss: 0.0178\n",
      "Epoch 93/100\n",
      "393/393 [==============================] - 0s 933us/step - loss: 0.0153 - val_loss: 0.0177\n",
      "Epoch 94/100\n",
      "393/393 [==============================] - 0s 922us/step - loss: 0.0153 - val_loss: 0.0175\n",
      "Epoch 95/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0153 - val_loss: 0.0178\n",
      "Epoch 96/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0153 - val_loss: 0.0175\n",
      "Epoch 97/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0153 - val_loss: 0.0177\n",
      "Epoch 98/100\n",
      "393/393 [==============================] - 0s 949us/step - loss: 0.0153 - val_loss: 0.0175\n",
      "Epoch 99/100\n",
      "393/393 [==============================] - 0s 1ms/step - loss: 0.0153 - val_loss: 0.0174\n",
      "Epoch 100/100\n",
      "393/393 [==============================] - 0s 955us/step - loss: 0.0152 - val_loss: 0.0174\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2db35945ba0>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "loss = MeanSquaredError()\n",
    "autoencoder.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "# Train the model\n",
    "real_headlines = X_train[y_train == 1]  # filter real news headlines\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "autoencoder.fit(real_headlines, real_headlines, epochs=epochs, batch_size=batch_size, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "02ab22c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145/145 [==============================] - 0s 672us/step\n"
     ]
    }
   ],
   "source": [
    "# Reconstruct headlines in the test set\n",
    "X_test_reconstructed = autoencoder.predict(X_test)\n",
    "\n",
    "# Calculate the reconstruction errors\n",
    "reconstruction_errors = np.mean((X_test - X_test_reconstructed)**2, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1443e606",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def find_optimal_threshold(errors, y_true):\n",
    "    min_diff = float('inf')\n",
    "    optimal_threshold = 0\n",
    "\n",
    "    for threshold in np.linspace(errors.min(), errors.max(), num=1000):\n",
    "        y_pred = (errors > threshold).astype(int)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        fpr = fp / (fp + tn)  # False Positive Rate\n",
    "        fnr = fn / (fn + tp)  # False Negative Rate\n",
    "\n",
    "        diff = abs(fpr - fnr)\n",
    "\n",
    "        if diff < min_diff:\n",
    "            min_diff = diff\n",
    "            optimal_threshold = threshold\n",
    "\n",
    "    return optimal_threshold\n",
    "\n",
    "threshold = find_optimal_threshold(reconstruction_errors, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3d842e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (reconstruction_errors > threshold).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "95fbc515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder Classifier Metrics:\n",
      "Accuracy: 0.5174568965517241\n",
      "Precision: 0.7593448131037379\n",
      "Recall: 0.5204375359815774\n",
      "F1 Score: 0.6175918018787361\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Autoencoder Classifier Metrics:\\nAccuracy: {accuracy}\\nPrecision: {precision}\\nRecall: {recall}\\nF1 Score: {f1}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6c254ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Metrics:\n",
      "Accuracy: 0.7676724137931035\n",
      "Precision: 0.7801683816651076\n",
      "Recall: 0.9602763385146805\n",
      "F1 Score: 0.8609032258064516\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train the logistic regression model with an increased number of iterations\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_log_reg = log_reg.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy_log_reg = accuracy_score(y_test, y_pred_log_reg)\n",
    "precision_log_reg = precision_score(y_test, y_pred_log_reg)\n",
    "recall_log_reg = recall_score(y_test, y_pred_log_reg)\n",
    "f1_log_reg = f1_score(y_test, y_pred_log_reg)\n",
    "\n",
    "print(f\"Logistic Regression Metrics:\\nAccuracy: {accuracy_log_reg}\\nPrecision: {precision_log_reg}\\nRecall: {recall_log_reg}\\nF1 Score: {f1_log_reg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "04ef7108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Metrics:\n",
      "Accuracy: 0.7887931034482759\n",
      "Precision: 0.7873271889400921\n",
      "Recall: 0.9835924006908463\n",
      "F1 Score: 0.8745840798566675\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train the random forest model\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "precision_rf = precision_score(y_test, y_pred_rf)\n",
    "recall_rf = recall_score(y_test, y_pred_rf)\n",
    "f1_rf = f1_score(y_test, y_pred_rf)\n",
    "\n",
    "print(f\"Random Forest Metrics:\\nAccuracy: {accuracy_rf}\\nPrecision: {precision_rf}\\nRecall: {recall_rf}\\nF1 Score: {f1_rf}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ba464c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e4aa37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442bc12d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00108ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
