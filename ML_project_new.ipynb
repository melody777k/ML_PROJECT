{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7cadc530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f01cc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "\n",
    "    # Convert text to lowercase and tokenize words\n",
    "    words = text.lower().split()\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Join words back into a single string\n",
    "    cleaned_text = ' '.join(words)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24679ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "fake_politifact = pd.read_csv('politifact_fake.csv')\n",
    "fake_gossipcop = pd.read_csv('gossipcop_fake.csv')\n",
    "abcnews = pd.read_csv('abcnews-date-text.csv')\n",
    "\n",
    "# Assign labels: 0 for fake news, 1 for real news\n",
    "fake_politifact['label'] = 0\n",
    "fake_gossipcop['label'] = 0\n",
    "abcnews['label'] = 1\n",
    "\n",
    "# Preprocess the dataset\n",
    "abcnews = abcnews.head(50000)  # Take the first 50,000 headlines\n",
    "abcnews.rename(columns={'headline_text': 'title'}, inplace=True)  # Rename the column to match other datasets\n",
    "abcnews['cleaned_title'] = abcnews['title'].apply(clean_text)  # Clean the headlines\n",
    "\n",
    "# Concatenate datasets\n",
    "data = pd.concat([fake_politifact, abcnews, fake_gossipcop], axis=0, ignore_index=True)\n",
    "data['cleaned_title'] = data['title'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd92cfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets (80% train, 20% test)\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ab5eefa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44604, 7)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b70117f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>news_url</th>\n",
       "      <th>title</th>\n",
       "      <th>tweet_ids</th>\n",
       "      <th>label</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>cleaned_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33488</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>coria beats massu to second successive title</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>20030728.0</td>\n",
       "      <td>coria beats massu second successive title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50577</th>\n",
       "      <td>gossipcop-4394027963</td>\n",
       "      <td>www.intouchweekly.com/posts/kourtney-kardashia...</td>\n",
       "      <td>Pregnant Kourtney Kardashian Hangs out With Ju...</td>\n",
       "      <td>821041155534098435\\t837002299952283649\\t837002...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pregnant kourtney kardashian hangs justin bieb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40281</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>man injured in two car collision on bass highway</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>20030830.0</td>\n",
       "      <td>man injured two car collision bass highway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24254</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>knights have last laugh in thriller</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>20030613.0</td>\n",
       "      <td>knights last laugh thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34623</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>aussies set the pace for team pursuit final</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>20030802.0</td>\n",
       "      <td>aussies set pace team pursuit final</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id   \n",
       "33488                   NaN  \\\n",
       "50577  gossipcop-4394027963   \n",
       "40281                   NaN   \n",
       "24254                   NaN   \n",
       "34623                   NaN   \n",
       "\n",
       "                                                news_url   \n",
       "33488                                                NaN  \\\n",
       "50577  www.intouchweekly.com/posts/kourtney-kardashia...   \n",
       "40281                                                NaN   \n",
       "24254                                                NaN   \n",
       "34623                                                NaN   \n",
       "\n",
       "                                                   title   \n",
       "33488       coria beats massu to second successive title  \\\n",
       "50577  Pregnant Kourtney Kardashian Hangs out With Ju...   \n",
       "40281   man injured in two car collision on bass highway   \n",
       "24254                knights have last laugh in thriller   \n",
       "34623        aussies set the pace for team pursuit final   \n",
       "\n",
       "                                               tweet_ids  label  publish_date   \n",
       "33488                                                NaN      1    20030728.0  \\\n",
       "50577  821041155534098435\\t837002299952283649\\t837002...      0           NaN   \n",
       "40281                                                NaN      1    20030830.0   \n",
       "24254                                                NaN      1    20030613.0   \n",
       "34623                                                NaN      1    20030802.0   \n",
       "\n",
       "                                           cleaned_title  \n",
       "33488          coria beats massu second successive title  \n",
       "50577  pregnant kourtney kardashian hangs justin bieb...  \n",
       "40281         man injured two car collision bass highway  \n",
       "24254                        knights last laugh thriller  \n",
       "34623                aussies set pace team pursuit final  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a6070f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11151, 7)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "afe12e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>news_url</th>\n",
       "      <th>title</th>\n",
       "      <th>tweet_ids</th>\n",
       "      <th>label</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>cleaned_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37535</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>diabetes on the rise but death rate drops</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>20030816.0</td>\n",
       "      <td>diabetes rise death rate drops</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3737</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>iraqi army has professional rapists vanstone</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>20030306.0</td>\n",
       "      <td>iraqi army professional rapists vanstone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55678</th>\n",
       "      <td>gossipcop-5035091613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jennifer Aniston   Brad Pitt Friends NOT Urg...</td>\n",
       "      <td>971088975438974977\\t971093257127002112\\t971130...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jennifer aniston brad pitt friends urging fair...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45581</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>roosters bulldogs clash sold out</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>20030926.0</td>\n",
       "      <td>roosters bulldogs clash sold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31592</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phelps plays down thorpe rivalry</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>20030718.0</td>\n",
       "      <td>phelps plays thorpe rivalry</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id news_url   \n",
       "37535                   NaN      NaN  \\\n",
       "3737                    NaN      NaN   \n",
       "55678  gossipcop-5035091613      NaN   \n",
       "45581                   NaN      NaN   \n",
       "31592                   NaN      NaN   \n",
       "\n",
       "                                                   title   \n",
       "37535          diabetes on the rise but death rate drops  \\\n",
       "3737        iraqi army has professional rapists vanstone   \n",
       "55678    Jennifer Aniston   Brad Pitt Friends NOT Urg...   \n",
       "45581                   roosters bulldogs clash sold out   \n",
       "31592                   phelps plays down thorpe rivalry   \n",
       "\n",
       "                                               tweet_ids  label  publish_date   \n",
       "37535                                                NaN      1    20030816.0  \\\n",
       "3737                                                 NaN      1    20030306.0   \n",
       "55678  971088975438974977\\t971093257127002112\\t971130...      0           NaN   \n",
       "45581                                                NaN      1    20030926.0   \n",
       "31592                                                NaN      1    20030718.0   \n",
       "\n",
       "                                           cleaned_title  \n",
       "37535                     diabetes rise death rate drops  \n",
       "3737            iraqi army professional rapists vanstone  \n",
       "55678  jennifer aniston brad pitt friends urging fair...  \n",
       "45581                       roosters bulldogs clash sold  \n",
       "31592                        phelps plays thorpe rivalry  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "34f8e0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_vectors(glove_file):\n",
    "    word_vectors = {}\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            word_vectors[word] = vector\n",
    "    return word_vectors\n",
    "\n",
    "glove_vectors = load_glove_vectors('glove.6B.50d.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "37b5e75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_average_vectors(docs, vectorizer, word_vectors, dim=50):\n",
    "    # Calculate the tf-idf weights for the given documents\n",
    "    tfidf_matrix = vectorizer.fit_transform(docs)\n",
    "\n",
    "    # Initialize an empty matrix to store the weighted average vectors\n",
    "    weighted_vectors = np.zeros((len(docs), dim))\n",
    "\n",
    "    # Iterate through the documents and compute the weighted average vector for each\n",
    "    for i, doc in enumerate(docs):\n",
    "        words = doc.split()\n",
    "        weighted_sum = np.zeros(dim)\n",
    "        total_weight = 0\n",
    "\n",
    "        for word in words:\n",
    "            if word in word_vectors and word in vectorizer.vocabulary_:\n",
    "                vector = word_vectors[word]\n",
    "                weight = tfidf_matrix[i, vectorizer.vocabulary_[word]]\n",
    "                weighted_sum += weight * vector\n",
    "                total_weight += weight\n",
    "\n",
    "        if total_weight != 0:\n",
    "            weighted_vectors[i] = weighted_sum / total_weight\n",
    "\n",
    "    return weighted_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6f562b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X_train = weighted_average_vectors(train_data['cleaned_title'], vectorizer, glove_vectors)\n",
    "X_test = weighted_average_vectors(test_data['cleaned_title'], vectorizer, glove_vectors)\n",
    "\n",
    "y_train = train_data['label'].values\n",
    "y_test = test_data['label'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f1f4514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_autoencoder(input_dim, encoding_dim, hidden_layers):\n",
    "    # Encoder\n",
    "    encoder_input = Input(shape=(input_dim,))\n",
    "    encoder_layers = [Dense(hidden_layers[0], activation='relu')(encoder_input)]\n",
    "\n",
    "    for layer_size in hidden_layers[1:]:\n",
    "        encoder_layers.append(Dense(layer_size, activation='relu')(encoder_layers[-1]))\n",
    "\n",
    "    # Encoded representation\n",
    "    encoded = Dense(encoding_dim, activation='relu')(encoder_layers[-1])\n",
    "\n",
    "    # Decoder\n",
    "    decoder_layers = [Dense(hidden_layers[-1], activation='relu')(encoded)]\n",
    "\n",
    "    for layer_size in reversed(hidden_layers[:-1]):\n",
    "        decoder_layers.append(Dense(layer_size, activation='relu')(decoder_layers[-1]))\n",
    "\n",
    "    # Reconstruction\n",
    "    decoder_output = Dense(input_dim, activation='linear')(decoder_layers[-1])\n",
    "\n",
    "    # Build the autoencoder model\n",
    "    autoencoder = Model(encoder_input, decoder_output)\n",
    "    return autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9930a651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 50)]              0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               6528      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 50)                6450      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 33,746\n",
      "Trainable params: 33,746\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "encoding_dim = 32\n",
    "hidden_layers = [128, 64]\n",
    "\n",
    "autoencoder = create_autoencoder(input_dim, encoding_dim, hidden_layers)\n",
    "autoencoder.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f2e46912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1126/1126 [==============================] - 2s 1ms/step - loss: 0.0447 - val_loss: 0.0287\n",
      "Epoch 2/100\n",
      "1126/1126 [==============================] - 2s 1ms/step - loss: 0.0278 - val_loss: 0.0269\n",
      "Epoch 3/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0266 - val_loss: 0.0261\n",
      "Epoch 4/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0246 - val_loss: 0.0240\n",
      "Epoch 5/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0237 - val_loss: 0.0236\n",
      "Epoch 6/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0231 - val_loss: 0.0228\n",
      "Epoch 7/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0227 - val_loss: 0.0226\n",
      "Epoch 8/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0224 - val_loss: 0.0226\n",
      "Epoch 9/100\n",
      "1126/1126 [==============================] - 2s 1ms/step - loss: 0.0221 - val_loss: 0.0229\n",
      "Epoch 10/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0219 - val_loss: 0.0222\n",
      "Epoch 11/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0217 - val_loss: 0.0220\n",
      "Epoch 12/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0215 - val_loss: 0.0215\n",
      "Epoch 13/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0213 - val_loss: 0.0215\n",
      "Epoch 14/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0212 - val_loss: 0.0215\n",
      "Epoch 15/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0211 - val_loss: 0.0213\n",
      "Epoch 16/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0210 - val_loss: 0.0213\n",
      "Epoch 17/100\n",
      "1126/1126 [==============================] - 2s 1ms/step - loss: 0.0208 - val_loss: 0.0211\n",
      "Epoch 18/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0208 - val_loss: 0.0209\n",
      "Epoch 19/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0207 - val_loss: 0.0211\n",
      "Epoch 20/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0206 - val_loss: 0.0208\n",
      "Epoch 21/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0205 - val_loss: 0.0207\n",
      "Epoch 22/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0204 - val_loss: 0.0204\n",
      "Epoch 23/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0203 - val_loss: 0.0209\n",
      "Epoch 24/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0202 - val_loss: 0.0204\n",
      "Epoch 25/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0202 - val_loss: 0.0204\n",
      "Epoch 26/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0201 - val_loss: 0.0204\n",
      "Epoch 27/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0201 - val_loss: 0.0200\n",
      "Epoch 28/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0200 - val_loss: 0.0202\n",
      "Epoch 29/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0200 - val_loss: 0.0205\n",
      "Epoch 30/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0199 - val_loss: 0.0203\n",
      "Epoch 31/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0199 - val_loss: 0.0204\n",
      "Epoch 32/100\n",
      "1126/1126 [==============================] - 2s 1ms/step - loss: 0.0198 - val_loss: 0.0201\n",
      "Epoch 33/100\n",
      "1126/1126 [==============================] - 2s 1ms/step - loss: 0.0198 - val_loss: 0.0204\n",
      "Epoch 34/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0197 - val_loss: 0.0198\n",
      "Epoch 35/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0197 - val_loss: 0.0200\n",
      "Epoch 36/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0197 - val_loss: 0.0200\n",
      "Epoch 37/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0196 - val_loss: 0.0199\n",
      "Epoch 38/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0196 - val_loss: 0.0199\n",
      "Epoch 39/100\n",
      "1126/1126 [==============================] - 2s 1ms/step - loss: 0.0196 - val_loss: 0.0203\n",
      "Epoch 40/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0195 - val_loss: 0.0201\n",
      "Epoch 41/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0195 - val_loss: 0.0198\n",
      "Epoch 42/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0194 - val_loss: 0.0199\n",
      "Epoch 43/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0194 - val_loss: 0.0196\n",
      "Epoch 44/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0194 - val_loss: 0.0199\n",
      "Epoch 45/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0193 - val_loss: 0.0197\n",
      "Epoch 46/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0193 - val_loss: 0.0197\n",
      "Epoch 47/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0193 - val_loss: 0.0199\n",
      "Epoch 48/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0192 - val_loss: 0.0196\n",
      "Epoch 49/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0192 - val_loss: 0.0198\n",
      "Epoch 50/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0192 - val_loss: 0.0196\n",
      "Epoch 51/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0191 - val_loss: 0.0197\n",
      "Epoch 52/100\n",
      "1126/1126 [==============================] - 2s 1ms/step - loss: 0.0191 - val_loss: 0.0196\n",
      "Epoch 53/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0191 - val_loss: 0.0195\n",
      "Epoch 54/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0191 - val_loss: 0.0193\n",
      "Epoch 55/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0190 - val_loss: 0.0193\n",
      "Epoch 56/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0190 - val_loss: 0.0193\n",
      "Epoch 57/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0190 - val_loss: 0.0196\n",
      "Epoch 58/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0190 - val_loss: 0.0192\n",
      "Epoch 59/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0190 - val_loss: 0.0194\n",
      "Epoch 60/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0189 - val_loss: 0.0192\n",
      "Epoch 61/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0189 - val_loss: 0.0195\n",
      "Epoch 62/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0189 - val_loss: 0.0191\n",
      "Epoch 63/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0189 - val_loss: 0.0192\n",
      "Epoch 64/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0188 - val_loss: 0.0191\n",
      "Epoch 65/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0188 - val_loss: 0.0191\n",
      "Epoch 66/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0188 - val_loss: 0.0191\n",
      "Epoch 67/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0188 - val_loss: 0.0190\n",
      "Epoch 68/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0187 - val_loss: 0.0191\n",
      "Epoch 69/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0187 - val_loss: 0.0191\n",
      "Epoch 70/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0187 - val_loss: 0.0193\n",
      "Epoch 71/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0187 - val_loss: 0.0193\n",
      "Epoch 72/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0187 - val_loss: 0.0194\n",
      "Epoch 73/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0187 - val_loss: 0.0192\n",
      "Epoch 74/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0187 - val_loss: 0.0192\n",
      "Epoch 75/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0186 - val_loss: 0.0190\n",
      "Epoch 76/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0186 - val_loss: 0.0194\n",
      "Epoch 77/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0186 - val_loss: 0.0189\n",
      "Epoch 78/100\n",
      "1126/1126 [==============================] - 2s 1ms/step - loss: 0.0186 - val_loss: 0.0189\n",
      "Epoch 79/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0186 - val_loss: 0.0189\n",
      "Epoch 80/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0185 - val_loss: 0.0189\n",
      "Epoch 81/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0185 - val_loss: 0.0190\n",
      "Epoch 82/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0185 - val_loss: 0.0190\n",
      "Epoch 83/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0185 - val_loss: 0.0189\n",
      "Epoch 84/100\n",
      "1126/1126 [==============================] - 2s 1ms/step - loss: 0.0185 - val_loss: 0.0191\n",
      "Epoch 85/100\n",
      "1126/1126 [==============================] - 2s 1ms/step - loss: 0.0184 - val_loss: 0.0190\n",
      "Epoch 86/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0185 - val_loss: 0.0190\n",
      "Epoch 87/100\n",
      "1126/1126 [==============================] - 2s 2ms/step - loss: 0.0184 - val_loss: 0.0188\n",
      "Epoch 88/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0184 - val_loss: 0.0188\n",
      "Epoch 89/100\n",
      "1126/1126 [==============================] - 2s 1ms/step - loss: 0.0184 - val_loss: 0.0187\n",
      "Epoch 90/100\n",
      "1126/1126 [==============================] - 2s 1ms/step - loss: 0.0184 - val_loss: 0.0187\n",
      "Epoch 91/100\n",
      "1126/1126 [==============================] - 2s 1ms/step - loss: 0.0184 - val_loss: 0.0188\n",
      "Epoch 92/100\n",
      "1126/1126 [==============================] - 2s 1ms/step - loss: 0.0184 - val_loss: 0.0189\n",
      "Epoch 93/100\n",
      "1126/1126 [==============================] - 2s 1ms/step - loss: 0.0183 - val_loss: 0.0185\n",
      "Epoch 94/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0183 - val_loss: 0.0188\n",
      "Epoch 95/100\n",
      "1126/1126 [==============================] - 1s 1ms/step - loss: 0.0183 - val_loss: 0.0187\n",
      "Epoch 96/100\n",
      "1126/1126 [==============================] - 2s 1ms/step - loss: 0.0183 - val_loss: 0.0188\n",
      "Epoch 97/100\n",
      "1126/1126 [==============================] - 2s 1ms/step - loss: 0.0183 - val_loss: 0.0188\n",
      "Epoch 98/100\n",
      "1126/1126 [==============================] - 2s 2ms/step - loss: 0.0183 - val_loss: 0.0190\n",
      "Epoch 99/100\n",
      "1126/1126 [==============================] - 2s 1ms/step - loss: 0.0183 - val_loss: 0.0189\n",
      "Epoch 100/100\n",
      "1126/1126 [==============================] - 2s 1ms/step - loss: 0.0182 - val_loss: 0.0189\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20628d06e30>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "loss = MeanSquaredError()\n",
    "autoencoder.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "# Train the model\n",
    "real_headlines = X_train[y_train == 1]  # filter real news headlines\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "autoencoder.fit(real_headlines, real_headlines, epochs=epochs, batch_size=batch_size, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a6d78342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "349/349 [==============================] - 0s 781us/step\n"
     ]
    }
   ],
   "source": [
    "# Reconstruct headlines in the test set\n",
    "X_test_reconstructed = autoencoder.predict(X_test)\n",
    "\n",
    "# Calculate the reconstruction errors\n",
    "reconstruction_errors = np.mean((X_test - X_test_reconstructed)**2, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0db7157d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def find_optimal_threshold(errors, y_true):\n",
    "    min_diff = float('inf')\n",
    "    optimal_threshold = 0\n",
    "\n",
    "    for threshold in np.linspace(errors.min(), errors.max(), num=1000):\n",
    "        y_pred = (errors > threshold).astype(int)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        fpr = fp / (fp + tn)  # False Positive Rate\n",
    "        fnr = fn / (fn + tp)  # False Negative Rate\n",
    "\n",
    "        diff = abs(fpr - fnr)\n",
    "\n",
    "        if diff < min_diff:\n",
    "            min_diff = diff\n",
    "            optimal_threshold = threshold\n",
    "\n",
    "    return optimal_threshold\n",
    "\n",
    "threshold = find_optimal_threshold(reconstruction_errors, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d0c0c054",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (reconstruction_errors > threshold).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "25cfdf4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder Classifier Metrics:\n",
      "Accuracy: 0.43063402385436284\n",
      "Precision: 0.8670169765561844\n",
      "Recall: 0.4298166516381124\n",
      "F1 Score: 0.5747203429566616\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Autoencoder Classifier Metrics:\\nAccuracy: {accuracy}\\nPrecision: {precision}\\nRecall: {recall}\\nF1 Score: {f1}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3dcfe5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Metrics:\n",
      "Accuracy: 0.9598242310106717\n",
      "Precision: 0.9699300009859017\n",
      "Recall: 0.9856727782787296\n",
      "F1 Score: 0.9777380242496521\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train the logistic regression model with an increased number of iterations\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_log_reg = log_reg.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy_log_reg = accuracy_score(y_test, y_pred_log_reg)\n",
    "precision_log_reg = precision_score(y_test, y_pred_log_reg)\n",
    "recall_log_reg = recall_score(y_test, y_pred_log_reg)\n",
    "f1_log_reg = f1_score(y_test, y_pred_log_reg)\n",
    "\n",
    "print(f\"Logistic Regression Metrics:\\nAccuracy: {accuracy_log_reg}\\nPrecision: {precision_log_reg}\\nRecall: {recall_log_reg}\\nF1 Score: {f1_log_reg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6331c5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Metrics:\n",
      "Accuracy: 0.9639494215765402\n",
      "Precision: 0.9652258377853327\n",
      "Recall: 0.995591624085763\n",
      "F1 Score: 0.9801736042611955\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train the random forest model\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "precision_rf = precision_score(y_test, y_pred_rf)\n",
    "recall_rf = recall_score(y_test, y_pred_rf)\n",
    "f1_rf = f1_score(y_test, y_pred_rf)\n",
    "\n",
    "print(f\"Random Forest Metrics:\\nAccuracy: {accuracy_rf}\\nPrecision: {precision_rf}\\nRecall: {recall_rf}\\nF1 Score: {f1_rf}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bc0f6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595df197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee1f11c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaec956",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705ce726",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
