{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cadc530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f01cc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "\n",
    "    # Convert text to lowercase and tokenize words\n",
    "    words = text.lower().split()\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Join words back into a single string\n",
    "    cleaned_text = ' '.join(words)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24679ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "fake_politifact = pd.read_csv('politifact_fake.csv')\n",
    "fake_gossipcop = pd.read_csv('gossipcop_fake.csv')\n",
    "abcnews = pd.read_csv('abcnews-date-text.csv')\n",
    "\n",
    "# Assign labels: 0 for fake news, 1 for real news\n",
    "fake_politifact['label'] = 0\n",
    "fake_gossipcop['label'] = 0\n",
    "abcnews['label'] = 1\n",
    "\n",
    "# Preprocess the dataset\n",
    "abcnews = abcnews.sample(5000)  # Take the first 5,000 headlines\n",
    "abcnews.rename(columns={'headline_text': 'title'}, inplace=True)  # Rename the column to match other datasets\n",
    "abcnews['cleaned_title'] = abcnews['title'].apply(clean_text)  # Clean the headlines\n",
    "\n",
    "# Concatenate datasets\n",
    "data = pd.concat([fake_politifact, abcnews, fake_gossipcop], axis=0, ignore_index=True)\n",
    "data['cleaned_title'] = data['title'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd92cfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets (80% train, 20% test)\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ab5eefa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8604, 7)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b70117f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>news_url</th>\n",
       "      <th>title</th>\n",
       "      <th>tweet_ids</th>\n",
       "      <th>label</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>cleaned_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9537</th>\n",
       "      <td>gossipcop-2717691939</td>\n",
       "      <td>www.townandcountrymag.com/society/a15845582/wh...</td>\n",
       "      <td>Latest News on Fired NBC Today Show Anchor</td>\n",
       "      <td>974979120537985024\\t1007020604099301377\\t10307...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>latest news fired nbc today show anchor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10425</th>\n",
       "      <td>gossipcop-7842742178</td>\n",
       "      <td>www.ok.co.uk/celebrity-news/1234596/margot-rob...</td>\n",
       "      <td>'Pregnant' Margot Robbie 'expecting child' wit...</td>\n",
       "      <td>816708608469663744\\t816708622432337924\\t816708...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pregnant margot robbie expecting child tom ack...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4339</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>firefighters hope weather aids efforts to tackle</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>20051011.0</td>\n",
       "      <td>firefighters hope weather aids efforts tackle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>politifact14116</td>\n",
       "      <td>www.usatoday.com/story/gameon/2012/11/12/lance...</td>\n",
       "      <td>Lance Armstrong's defiant Twitter photo shows ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lance armstrong defiant twitter photo shows re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>melbourne man hospitalised after stabbing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>20070325.0</td>\n",
       "      <td>melbourne man hospitalised stabbing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id  \\\n",
       "9537   gossipcop-2717691939   \n",
       "10425  gossipcop-7842742178   \n",
       "4339                    NaN   \n",
       "353         politifact14116   \n",
       "456                     NaN   \n",
       "\n",
       "                                                news_url  \\\n",
       "9537   www.townandcountrymag.com/society/a15845582/wh...   \n",
       "10425  www.ok.co.uk/celebrity-news/1234596/margot-rob...   \n",
       "4339                                                 NaN   \n",
       "353    www.usatoday.com/story/gameon/2012/11/12/lance...   \n",
       "456                                                  NaN   \n",
       "\n",
       "                                                   title  \\\n",
       "9537          Latest News on Fired NBC Today Show Anchor   \n",
       "10425  'Pregnant' Margot Robbie 'expecting child' wit...   \n",
       "4339    firefighters hope weather aids efforts to tackle   \n",
       "353    Lance Armstrong's defiant Twitter photo shows ...   \n",
       "456            melbourne man hospitalised after stabbing   \n",
       "\n",
       "                                               tweet_ids  label  publish_date  \\\n",
       "9537   974979120537985024\\t1007020604099301377\\t10307...      0           NaN   \n",
       "10425  816708608469663744\\t816708622432337924\\t816708...      0           NaN   \n",
       "4339                                                 NaN      1    20051011.0   \n",
       "353                                                  NaN      0           NaN   \n",
       "456                                                  NaN      1    20070325.0   \n",
       "\n",
       "                                           cleaned_title  \n",
       "9537             latest news fired nbc today show anchor  \n",
       "10425  pregnant margot robbie expecting child tom ack...  \n",
       "4339       firefighters hope weather aids efforts tackle  \n",
       "353    lance armstrong defiant twitter photo shows re...  \n",
       "456                  melbourne man hospitalised stabbing  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a6070f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2151, 7)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afe12e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>news_url</th>\n",
       "      <th>title</th>\n",
       "      <th>tweet_ids</th>\n",
       "      <th>label</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>cleaned_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9043</th>\n",
       "      <td>gossipcop-8776120919</td>\n",
       "      <td>hollywoodlife.com/2017/07/28/george-clooney-te...</td>\n",
       "      <td>George Clooney Terrified Over Safety Of His Tw...</td>\n",
       "      <td>891131523562426369\\t891132638261649408\\t891133...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>george clooney terrified safety twins photogra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>study to investigate flood protection measures</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>20051107.0</td>\n",
       "      <td>study investigate flood protection measures</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6044</th>\n",
       "      <td>gossipcop-5750046920</td>\n",
       "      <td>www.accessonline.com/articles/ariel-winters-bo...</td>\n",
       "      <td>Ariel Winter's Boyfriend Levi Meaden Cheekily ...</td>\n",
       "      <td>939265361869930496\\t939266680051716096\\t939366...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ariel winter boyfriend levi meaden cheekily gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10720</th>\n",
       "      <td>gossipcop-6421829445</td>\n",
       "      <td>www.telegraph.co.uk/news/2017/07/27/justin-bie...</td>\n",
       "      <td>Justin Bieber accidentally hits photographer w...</td>\n",
       "      <td>890469931137081344\\t890477494763114496\\t890480...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>justin bieber accidentally hits photographer p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10196</th>\n",
       "      <td>gossipcop-9533859944</td>\n",
       "      <td>metro.co.uk/2018/04/16/nicole-kidman-wants-kei...</td>\n",
       "      <td>Nicole Kidman wants Keith Urban to join cast o...</td>\n",
       "      <td>967193199151386625</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nicole kidman wants keith urban join cast big ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id  \\\n",
       "9043   gossipcop-8776120919   \n",
       "1501                    NaN   \n",
       "6044   gossipcop-5750046920   \n",
       "10720  gossipcop-6421829445   \n",
       "10196  gossipcop-9533859944   \n",
       "\n",
       "                                                news_url  \\\n",
       "9043   hollywoodlife.com/2017/07/28/george-clooney-te...   \n",
       "1501                                                 NaN   \n",
       "6044   www.accessonline.com/articles/ariel-winters-bo...   \n",
       "10720  www.telegraph.co.uk/news/2017/07/27/justin-bie...   \n",
       "10196  metro.co.uk/2018/04/16/nicole-kidman-wants-kei...   \n",
       "\n",
       "                                                   title  \\\n",
       "9043   George Clooney Terrified Over Safety Of His Tw...   \n",
       "1501      study to investigate flood protection measures   \n",
       "6044   Ariel Winter's Boyfriend Levi Meaden Cheekily ...   \n",
       "10720  Justin Bieber accidentally hits photographer w...   \n",
       "10196  Nicole Kidman wants Keith Urban to join cast o...   \n",
       "\n",
       "                                               tweet_ids  label  publish_date  \\\n",
       "9043   891131523562426369\\t891132638261649408\\t891133...      0           NaN   \n",
       "1501                                                 NaN      1    20051107.0   \n",
       "6044   939265361869930496\\t939266680051716096\\t939366...      0           NaN   \n",
       "10720  890469931137081344\\t890477494763114496\\t890480...      0           NaN   \n",
       "10196                                 967193199151386625      0           NaN   \n",
       "\n",
       "                                           cleaned_title  \n",
       "9043   george clooney terrified safety twins photogra...  \n",
       "1501         study investigate flood protection measures  \n",
       "6044   ariel winter boyfriend levi meaden cheekily gr...  \n",
       "10720  justin bieber accidentally hits photographer p...  \n",
       "10196  nicole kidman wants keith urban join cast big ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34f8e0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_vectors(glove_file):\n",
    "    word_vectors = {}\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            word_vectors[word] = vector\n",
    "    return word_vectors\n",
    "\n",
    "glove_vectors = load_glove_vectors('glove.6B.50d.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37b5e75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_average_vectors(docs, vectorizer, word_vectors, dim=50):\n",
    "    # Calculate the tf-idf weights for the given documents\n",
    "    tfidf_matrix = vectorizer.fit_transform(docs)\n",
    "\n",
    "    # Initialize an empty matrix to store the weighted average vectors\n",
    "    weighted_vectors = np.zeros((len(docs), dim))\n",
    "\n",
    "    # Iterate through the documents and compute the weighted average vector for each\n",
    "    for i, doc in enumerate(docs):\n",
    "        words = doc.split()\n",
    "        weighted_sum = np.zeros(dim)\n",
    "        total_weight = 0\n",
    "\n",
    "        for word in words:\n",
    "            if word in word_vectors and word in vectorizer.vocabulary_:\n",
    "                vector = word_vectors[word]\n",
    "                weight = tfidf_matrix[i, vectorizer.vocabulary_[word]]\n",
    "                weighted_sum += weight * vector\n",
    "                total_weight += weight\n",
    "\n",
    "        if total_weight != 0:\n",
    "            weighted_vectors[i] = weighted_sum / total_weight\n",
    "\n",
    "    return weighted_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6f562b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X_train = weighted_average_vectors(train_data['cleaned_title'], vectorizer, glove_vectors)\n",
    "X_test = weighted_average_vectors(test_data['cleaned_title'], vectorizer, glove_vectors)\n",
    "\n",
    "y_train = train_data['label'].values\n",
    "y_test = test_data['label'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f1f4514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_autoencoder(input_dim, encoding_dim, hidden_layers):\n",
    "    # Encoder\n",
    "    encoder_input = Input(shape=(input_dim,))\n",
    "    encoder_layers = [Dense(hidden_layers[0], activation='relu')(encoder_input)]\n",
    "\n",
    "    for layer_size in hidden_layers[1:]:\n",
    "        encoder_layers.append(Dense(layer_size, activation='relu')(encoder_layers[-1]))\n",
    "\n",
    "    # Encoded representation\n",
    "    encoded = Dense(encoding_dim, activation='relu')(encoder_layers[-1])\n",
    "\n",
    "    # Decoder\n",
    "    decoder_layers = [Dense(hidden_layers[-1], activation='relu')(encoded)]\n",
    "\n",
    "    for layer_size in reversed(hidden_layers[:-1]):\n",
    "        decoder_layers.append(Dense(layer_size, activation='relu')(decoder_layers[-1]))\n",
    "\n",
    "    # Reconstruction\n",
    "    decoder_output = Dense(input_dim, activation='linear')(decoder_layers[-1])\n",
    "\n",
    "    # Build the autoencoder model\n",
    "    autoencoder = Model(encoder_input, decoder_output)\n",
    "    return autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9930a651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 50)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               6528      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 50)                6450      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 33,746\n",
      "Trainable params: 33,746\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "encoding_dim = 32\n",
    "hidden_layers = [128, 64]\n",
    "\n",
    "autoencoder = create_autoencoder(input_dim, encoding_dim, hidden_layers)\n",
    "autoencoder.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2e46912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "113/113 [==============================] - 2s 10ms/step - loss: 0.1062 - val_loss: 0.0698\n",
      "Epoch 2/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0591 - val_loss: 0.0499\n",
      "Epoch 3/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0456 - val_loss: 0.0420\n",
      "Epoch 4/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0395 - val_loss: 0.0383\n",
      "Epoch 5/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0354 - val_loss: 0.0344\n",
      "Epoch 6/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0324 - val_loss: 0.0323\n",
      "Epoch 7/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0306 - val_loss: 0.0314\n",
      "Epoch 8/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0296 - val_loss: 0.0303\n",
      "Epoch 9/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0289 - val_loss: 0.0298\n",
      "Epoch 10/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0283 - val_loss: 0.0299\n",
      "Epoch 11/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0278 - val_loss: 0.0291\n",
      "Epoch 12/100\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 0.0275 - val_loss: 0.0288\n",
      "Epoch 13/100\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 0.0271 - val_loss: 0.0291\n",
      "Epoch 14/100\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 0.0267 - val_loss: 0.0283\n",
      "Epoch 15/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0265 - val_loss: 0.0284\n",
      "Epoch 16/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0261 - val_loss: 0.0277\n",
      "Epoch 17/100\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 0.0253 - val_loss: 0.0269\n",
      "Epoch 18/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0247 - val_loss: 0.0261\n",
      "Epoch 19/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0243 - val_loss: 0.0259\n",
      "Epoch 20/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0239 - val_loss: 0.0256\n",
      "Epoch 21/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0231 - val_loss: 0.0252\n",
      "Epoch 22/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0228 - val_loss: 0.0252\n",
      "Epoch 23/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0225 - val_loss: 0.0249\n",
      "Epoch 24/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0223 - val_loss: 0.0250\n",
      "Epoch 25/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0221 - val_loss: 0.0248\n",
      "Epoch 26/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0220 - val_loss: 0.0243\n",
      "Epoch 27/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0218 - val_loss: 0.0243\n",
      "Epoch 28/100\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 0.0216 - val_loss: 0.0244\n",
      "Epoch 29/100\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 0.0215 - val_loss: 0.0242\n",
      "Epoch 30/100\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 0.0214 - val_loss: 0.0245\n",
      "Epoch 31/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0214 - val_loss: 0.0243\n",
      "Epoch 32/100\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 0.0213 - val_loss: 0.0243\n",
      "Epoch 33/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0211 - val_loss: 0.0242\n",
      "Epoch 34/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0211 - val_loss: 0.0240\n",
      "Epoch 35/100\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 0.0209 - val_loss: 0.0239\n",
      "Epoch 36/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0209 - val_loss: 0.0239\n",
      "Epoch 37/100\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 0.0209 - val_loss: 0.0236\n",
      "Epoch 38/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0209 - val_loss: 0.0238\n",
      "Epoch 39/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0206 - val_loss: 0.0239\n",
      "Epoch 40/100\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 0.0205 - val_loss: 0.0236\n",
      "Epoch 41/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0205 - val_loss: 0.0239\n",
      "Epoch 42/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0205 - val_loss: 0.0235\n",
      "Epoch 43/100\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 0.0206 - val_loss: 0.0240\n",
      "Epoch 44/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0205 - val_loss: 0.0237\n",
      "Epoch 45/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0204 - val_loss: 0.0239\n",
      "Epoch 46/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0203 - val_loss: 0.0239\n",
      "Epoch 47/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0203 - val_loss: 0.0234\n",
      "Epoch 48/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0202 - val_loss: 0.0242\n",
      "Epoch 49/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0201 - val_loss: 0.0237\n",
      "Epoch 50/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0201 - val_loss: 0.0239\n",
      "Epoch 51/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0203 - val_loss: 0.0239\n",
      "Epoch 52/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0200 - val_loss: 0.0234\n",
      "Epoch 53/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0199 - val_loss: 0.0237\n",
      "Epoch 54/100\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 0.0198 - val_loss: 0.0234\n",
      "Epoch 55/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0198 - val_loss: 0.0233\n",
      "Epoch 56/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0198 - val_loss: 0.0234\n",
      "Epoch 57/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0197 - val_loss: 0.0236\n",
      "Epoch 58/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0197 - val_loss: 0.0238\n",
      "Epoch 59/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0197 - val_loss: 0.0238\n",
      "Epoch 60/100\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 0.0196 - val_loss: 0.0235\n",
      "Epoch 61/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0195 - val_loss: 0.0235\n",
      "Epoch 62/100\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 0.0197 - val_loss: 0.0234\n",
      "Epoch 63/100\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 0.0196 - val_loss: 0.0235\n",
      "Epoch 64/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0194 - val_loss: 0.0232\n",
      "Epoch 65/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0194 - val_loss: 0.0229\n",
      "Epoch 66/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0195 - val_loss: 0.0233\n",
      "Epoch 67/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0194 - val_loss: 0.0229\n",
      "Epoch 68/100\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 0.0194 - val_loss: 0.0231\n",
      "Epoch 69/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0194 - val_loss: 0.0230\n",
      "Epoch 70/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0194 - val_loss: 0.0233\n",
      "Epoch 71/100\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 0.0192 - val_loss: 0.0231\n",
      "Epoch 72/100\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 0.0194 - val_loss: 0.0230\n",
      "Epoch 73/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0192 - val_loss: 0.0231\n",
      "Epoch 74/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0191 - val_loss: 0.0232\n",
      "Epoch 75/100\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 0.0192 - val_loss: 0.0230\n",
      "Epoch 76/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0192 - val_loss: 0.0232\n",
      "Epoch 77/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0192 - val_loss: 0.0231\n",
      "Epoch 78/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0191 - val_loss: 0.0233\n",
      "Epoch 79/100\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 0.0190 - val_loss: 0.0230\n",
      "Epoch 80/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0190 - val_loss: 0.0230\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0190 - val_loss: 0.0229\n",
      "Epoch 82/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0189 - val_loss: 0.0232\n",
      "Epoch 83/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0190 - val_loss: 0.0232\n",
      "Epoch 84/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0189 - val_loss: 0.0230\n",
      "Epoch 85/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0190 - val_loss: 0.0229\n",
      "Epoch 86/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0189 - val_loss: 0.0228\n",
      "Epoch 87/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0189 - val_loss: 0.0231\n",
      "Epoch 88/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0189 - val_loss: 0.0226\n",
      "Epoch 89/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0188 - val_loss: 0.0232\n",
      "Epoch 90/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0189 - val_loss: 0.0233\n",
      "Epoch 91/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0188 - val_loss: 0.0232\n",
      "Epoch 92/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0188 - val_loss: 0.0230\n",
      "Epoch 93/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0187 - val_loss: 0.0234\n",
      "Epoch 94/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0188 - val_loss: 0.0232\n",
      "Epoch 95/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0187 - val_loss: 0.0231\n",
      "Epoch 96/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0188 - val_loss: 0.0231\n",
      "Epoch 97/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0187 - val_loss: 0.0229\n",
      "Epoch 98/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0186 - val_loss: 0.0233\n",
      "Epoch 99/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0186 - val_loss: 0.0230\n",
      "Epoch 100/100\n",
      "113/113 [==============================] - 0s 4ms/step - loss: 0.0186 - val_loss: 0.0231\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16b073e64a0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "loss = MeanSquaredError()\n",
    "autoencoder.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "# Train the model\n",
    "real_headlines = X_train[y_train == 1]  # filter real news headlines\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "autoencoder.fit(real_headlines, real_headlines, epochs=epochs, batch_size=batch_size, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6d78342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Reconstruct headlines in the test set\n",
    "X_test_reconstructed = autoencoder.predict(X_test)\n",
    "\n",
    "# Calculate the reconstruction errors\n",
    "reconstruction_errors = np.mean((X_test - X_test_reconstructed)**2, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0db7157d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def find_optimal_threshold(errors, y_true):\n",
    "    min_diff = float('inf')\n",
    "    optimal_threshold = 0\n",
    "\n",
    "    for threshold in np.linspace(errors.min(), errors.max(), num=1000):\n",
    "        y_pred = (errors > threshold).astype(int)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        fpr = fp / (fp + tn)  # False Positive Rate\n",
    "        fnr = fn / (fn + tp)  # False Negative Rate\n",
    "\n",
    "        diff = abs(fpr - fnr)\n",
    "\n",
    "        if diff < min_diff:\n",
    "            min_diff = diff\n",
    "            optimal_threshold = threshold\n",
    "\n",
    "    return optimal_threshold\n",
    "\n",
    "threshold = find_optimal_threshold(reconstruction_errors, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0c0c054",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (reconstruction_errors > threshold).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25cfdf4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder Classifier Metrics:\n",
      "Accuracy: 0.4909344490934449\n",
      "Precision: 0.4553072625698324\n",
      "Recall: 0.4894894894894895\n",
      "F1 Score: 0.47178002894356\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Autoencoder Classifier Metrics:\\nAccuracy: {accuracy}\\nPrecision: {precision}\\nRecall: {recall}\\nF1 Score: {f1}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3dcfe5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Metrics:\n",
      "Accuracy: 0.899116689911669\n",
      "Precision: 0.895748987854251\n",
      "Recall: 0.8858858858858859\n",
      "F1 Score: 0.890790135883241\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train the logistic regression model with an increased number of iterations\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_log_reg = log_reg.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy_log_reg = accuracy_score(y_test, y_pred_log_reg)\n",
    "precision_log_reg = precision_score(y_test, y_pred_log_reg)\n",
    "recall_log_reg = recall_score(y_test, y_pred_log_reg)\n",
    "f1_log_reg = f1_score(y_test, y_pred_log_reg)\n",
    "\n",
    "print(f\"Logistic Regression Metrics:\\nAccuracy: {accuracy_log_reg}\\nPrecision: {precision_log_reg}\\nRecall: {recall_log_reg}\\nF1 Score: {f1_log_reg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6331c5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Metrics:\n",
      "Accuracy: 0.900976290097629\n",
      "Precision: 0.882295719844358\n",
      "Recall: 0.9079079079079079\n",
      "F1 Score: 0.8949185989146522\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train the random forest model\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "precision_rf = precision_score(y_test, y_pred_rf)\n",
    "recall_rf = recall_score(y_test, y_pred_rf)\n",
    "f1_rf = f1_score(y_test, y_pred_rf)\n",
    "\n",
    "print(f\"Random Forest Metrics:\\nAccuracy: {accuracy_rf}\\nPrecision: {precision_rf}\\nRecall: {recall_rf}\\nF1 Score: {f1_rf}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bc0f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train the random forest model\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "precision_rf = precision_score(y_test, y_pred_rf)\n",
    "recall_rf = recall_score(y_test, y_pred_rf)\n",
    "f1_rf = f1_score(y_test, y_pred_rf)\n",
    "\n",
    "print(f\"Random Forest Metrics:\\nAccuracy: {accuracy_rf}\\nPrecision: {precision_rf}\\nRecall: {recall_rf}\\nF1 Score: {f1_rf}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595df197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eee1f11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP:\n",
      "Accuracy: 0.905625290562529\n",
      "Precision: 0.8940594059405941\n",
      "Recall: 0.9039039039039038\n",
      "F1 Score: 0.8989547038327527\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#Train\n",
    "mlp = MLPClassifier(solver='sgd', learning_rate = \"adaptive\", learning_rate_init = 0.1, alpha=1e-5,hidden_layer_sizes=(64,32), random_state=1,max_iter=500)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "#Predict\n",
    "y_pred_mlp = mlp.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy_mlp = accuracy_score(y_test, y_pred_mlp)\n",
    "precision_mlp = precision_score(y_test, y_pred_mlp)\n",
    "recall_mlp = recall_score(y_test, y_pred_mlp)\n",
    "f1_mlp = f1_score(y_test, y_pred_mlp)\n",
    "\n",
    "print(f\"MLP:\\nAccuracy: {accuracy_mlp}\\nPrecision: {precision_mlp}\\nRecall: {recall_mlp}\\nF1 Score: {f1_mlp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaec956",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "705ce726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM:\n",
      "Accuracy: 0.898186889818689\n",
      "Precision: 0.8963414634146342\n",
      "Recall: 0.8828828828828829\n",
      "F1 Score: 0.8895612708018154\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "#Train\n",
    "svc = LinearSVC(penalty = \"l1\", loss=\"squared_hinge\",dual=False, max_iter = 10000, tol = 0.001)\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "#Predict\n",
    "y_pred_svc = svc.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy_svc = accuracy_score(y_test, y_pred_svc)\n",
    "precision_svc = precision_score(y_test, y_pred_svc)\n",
    "recall_svc = recall_score(y_test, y_pred_svc)\n",
    "f1_svc = f1_score(y_test, y_pred_svc)\n",
    "\n",
    "print(f\"SVM:\\nAccuracy: {accuracy_svc}\\nPrecision: {precision_svc}\\nRecall: {recall_svc}\\nF1 Score: {f1_svc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb2ecb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "651a24c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree:\n",
      "Accuracy: 0.8303114830311483\n",
      "Precision: 0.8163672654690619\n",
      "Recall: 0.8188188188188188\n",
      "F1 Score: 0.8175912043978012\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#Train\n",
    "dt = DecisionTreeClassifier(random_state=0)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "#Predict\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
    "precision_dt = precision_score(y_test, y_pred_dt)\n",
    "recall_dt = recall_score(y_test, y_pred_dt)\n",
    "f1_dt = f1_score(y_test, y_pred_dt)\n",
    "\n",
    "print(f\"Decision Tree:\\nAccuracy: {accuracy_dt}\\nPrecision: {precision_dt}\\nRecall: {recall_dt}\\nF1 Score: {f1_dt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a4c5e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "82916eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN:\n",
      "Accuracy: 0.897721989772199\n",
      "Precision: 0.91129883843717\n",
      "Recall: 0.8638638638638638\n",
      "F1 Score: 0.8869475847893115\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#Train\n",
    "knn = KNeighborsClassifier(n_neighbors=5, metric=\"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "#Predict\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "precision_knn = precision_score(y_test, y_pred_knn)\n",
    "recall_knn = recall_score(y_test, y_pred_knn)\n",
    "f1_knn = f1_score(y_test, y_pred_knn)\n",
    "\n",
    "print(f\"KNN:\\nAccuracy: {accuracy_knn}\\nPrecision: {precision_knn}\\nRecall: {recall_knn}\\nF1 Score: {f1_knn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6ffe18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5c4f5275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree:\n",
      "Accuracy: 0.9121338912133892\n",
      "Precision: 0.905811623246493\n",
      "Recall: 0.9049049049049049\n",
      "F1 Score: 0.9053580370555834\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Train\n",
    "rf = RandomForestClassifier(max_depth=10, random_state=0)\n",
    "ada = AdaBoostClassifier(estimator = rf, n_estimators=10)\n",
    "ada.fit(X_train, y_train)\n",
    "\n",
    "#Predict\n",
    "y_pred_ada = ada.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy_ada = accuracy_score(y_test, y_pred_ada)\n",
    "precision_ada = precision_score(y_test, y_pred_ada)\n",
    "recall_ada = recall_score(y_test, y_pred_ada)\n",
    "f1_ada = f1_score(y_test, y_pred_ada)\n",
    "\n",
    "print(f\"Decision Tree:\\nAccuracy: {accuracy_ada}\\nPrecision: {precision_ada}\\nRecall: {recall_ada}\\nF1 Score: {f1_ada}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
